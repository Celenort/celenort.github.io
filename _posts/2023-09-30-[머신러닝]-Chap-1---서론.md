---
layout: post
date: 2023-09-30
title: "[ë¨¸ì‹ ëŸ¬ë‹] Chap 1 - ì„œë¡ "
tags: [ml, from_velog, abstract, ]
categories: [Machine Learning, ml_lecture, ]
media_subpath: assets/img/2023-09-30-[ë¨¸ì‹ ëŸ¬ë‹]-Chap-1---ì„œë¡ .md
image:
  path: 0.png
  alt: ${alt}
description: ë¨¸ì‹ ëŸ¬ë‹ì˜ ê¸°ë³¸ ìš©ì–´ì™€ ê°œë…ì„ ì„¤ëª…í•˜ë©°, ë°ì´í„° ì„¸íŠ¸, ì¸ìŠ¤í„´ìŠ¤, ì†ì„±, ê°€ì„¤ ê³µê°„, ê·€ë‚©ì  í•™ìŠµ, ê·¸ë¦¬ê³  ì•Œê³ ë¦¬ì¦˜ì˜ í¸í–¥ì— ëŒ€í•´ ë…¼ì˜í•©ë‹ˆë‹¤. 80ë…„ëŒ€ ì¼ë³¸ ê²½ì œì™€ ê´€ë ¨ëœ ì£¼ìš” ìš”ì†Œì¸ ë²„ë¸” ê²½ì œì™€ í”Œë¼ì í•©ì˜ì˜ ì˜í–¥ì„ ì„¤ëª…í•˜ê³ , ë¨¸ì‹ ëŸ¬ë‹ì˜ ëª©í‘œëŠ” ì¢‹ì€ ì¼ë°˜í™” ëŠ¥ë ¥ì„ ê°–ì¶”ëŠ” ê²ƒì„ì„ ê°•ì¡°í•©ë‹ˆë‹¤.
pin: false
---


### Disclaimer


{: .prompt-info }


> ğŸ“£ ë³¸ í¬ìŠ¤íŠ¸ëŠ” ì¡°ìš°ì¯”í™”ì˜ [ë‹¨ë‹¨í•œ ë¨¸ì‹ ëŸ¬ë‹](https://product.kyobobook.co.kr/detail/S000001916959) ì±…ì„ ìš”ì•½ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤. 


# Chap 1. ì„œë¡ 


## 1.2 Machine Learningì˜ ê¸°ë³¸ ìš©ì–´


data set
: ê° ì‚¬ë¬¼ì— ëŒ€í•œ ë¬˜ì‚¬(instance, sample)ì˜ ì§‘í•©


instance, sample
: íŠ¹ì • ì‚¬ë¬¼ì— ëŒ€í•œ ë¬˜ì‚¬ (atrributeë¥¼ ì´ìš©í•˜ì—¬)


attribute, feature
: íŠ¹ì • ì‚¬ë¬¼ì˜ ì†ì„±, ì„±ì§ˆ


attribute value
: ê·¸ ì†ì„±ì— ëŒ€í•œ ì†ì„±ê°’


attribute space (sample, input space)
: ì†ì„±ë“¤ì„ ê³µê°„ì— í™•ì¥ì‹œí‚´


feature vector
: íŠ¹ì • sampleì˜ featureë“¤ì„ spaceì— ë‚˜íƒ€ë‚´ì—ˆì„ ë•Œ í•˜ë‚˜ì˜ sampleì´ ê°€ë¦¬í‚¤ëŠ” vector


Machine Learning
: Learn from experience(Data) E, in some tasks T measured by performance measure P


Label
: sampleì˜ ê²°ê³¼ê°’ì„ ë‚˜íƒ€ë‚´ëŠ” ì •ë³´, labelì— ë”°ë¼ ë¬¸ì œì˜ ì¢…ë¥˜ê°€ ê²°ì •ë¨

- classsification (ë¶„ë¥˜)
:  ì´ì‚°ê°’ì˜ ê²½ìš° // supervised learning
- regression (íšŒê·€) 
: ì—°ì†ê°’ì˜ ê²½ìš° // supervised learning
- clustering (í´ëŸ¬ìŠ¤í„°ë§)
: ì´ì‚°ê°’, ë¶„ë¥˜ ê¸°ì¤€ì´ ìë™ì ìœ¼ë¡œ í˜•ì„± (ì‚¬ì „ì— ì•Œ ìˆ˜ ì—†ìŒ) // unsupervised learning

testing
: í•™ìŠµìœ¼ë¡œ ë§Œë“  ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ì˜ˆì¸¡í•˜ëŠ” ê²ƒ. ê²€ì¦


testing sample (set) 
: ê²€ì¦ì— í™œìš©í•˜ëŠ” sampleë“¤ë¡œ, trainingì—ëŠ” ë“±ì¥í•˜ì§€ ì•Šì•˜ë˜ ê²ƒë“¤


training sample (set)
: í•™ìŠµì— ì‚¬ìš©ë˜ëŠ” sampleë“¤

- goals of ML : good "generalization" ability. (testing setì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ëŠ” ê²ƒ)
- sampleë“¤ì— ëŒ€í•œ ê°€ì • : all samples are i.i.d (independent, identically distributed)
- generally speaking, more sample, the better generalized model

## 1.3 Hypothesis space


ê·€ë‚© 
: êµ¬ì²´ì  ì‚¬ì‹¤ì´ë‚˜ íŠ¹ìˆ˜ì„±ìœ¼ë¡œë¶€í„° ì¼ë°˜ì„±ì„ ì°¾ìŒ.


ì—°ì—­ 
: ì¼ë°˜ì  ë²•ì¹™, ê³µë¦¬ë¡œë¶€í„° íŠ¹ìˆ˜í•œ ì •í™©ì„ ì¶”ë¡ 

- Machine Learningì€ sampleì„ í†µí•´ í•™ìŠµí•¨ìœ¼ë¡œ ê·€ë‚©ì˜ ì›ë¦¬ë¥¼ ì‚¬ìš©í•œ ê·€ë‚© í•™ìŠµ(inductive learning) ì´ë¼ê³  í•  ìˆ˜ ìˆìŒ.
- attributeë¥¼ ì´ìš©í•˜ì—¬ hypothesis spaceë¥¼ êµ¬ì„± ê°€ëŠ¥.

Hypothesis space 
: ì‚¬ì‹¤ìƒ ê°€ëŠ¥í•œ ëª¨ë“  ê°€ì„¤ë“¤ì˜ ì´ì§‘í•©. attribute ë³„ë¡œ ê°€ëŠ¥í•œ attribute valueì˜ ê°€ì§“ìˆ˜ + 1 (don't care) ë¥¼ ê·€ë‚©ì ìœ¼ë¡œ ë‚˜ì—´í•œ ê²ƒ

- eg) "ì˜ ìµì€ ìˆ˜ë°•"ì˜ íŒë³„ì„ ìœ„í•´ ìƒ‰ê¹”, ê¼­ì§€, ì†Œë¦¬ë¼ëŠ” attributeë¡œ hypothesis spaceë¥¼ ë§Œë“ ë‹¤ê³  ê°€ì •í•˜ì.
	- ê°ê°ì˜ attributeëŠ” 3ê°œì˜ attribute valueë¥¼ ê°€ì§ˆ ìˆ˜ ìˆë‹¤ê³  í•˜ë©´,
	hypothesis spaceì˜ í¬ê¸°ëŠ” ì´ $(3+1)\cdot(3+1)\cdot(3+1)+1$ ê°œì¼ ê²ƒì´ë‹¤.
	ê´„í˜¸ ë‚´ì—ì„œ í•˜ë‚˜ì”© ë”í•´ì§„ 1ì€ "\*" ì„ ë‚˜íƒ€ë‚´ëŠ” ê²ƒìœ¼ë¡œ don't care, í•´ë‹¹ attributeëŠ” ë¬´ê´€í•¨
	ë§ˆì§€ë§‰ì˜ +1ì€ 'ì˜ ìµì€ ìˆ˜ë°•'ì´ë¼ëŠ” ê°œë… ìì²´ê°€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°ì— ëŒ€í•œ hypothesisë¼ê³  í•œë‹¤.

![0](/0.png)


Version space 
: ë„ˆë¬´ë‚˜ ë§ì€ ê°€ì„¤ë“¤ì´ training setì— ë¶€í•©í•  ê²½ìš° ê·¸ ê°€ì„¤ë“¤ì˜ ì§‘í•©ì„ ì˜ë¯¸í•¨.


![1](/1.png)


## 1.4 Inductive bias

- ì•Œê³ ë¦¬ì¦˜ì´ íŠ¹ì • ìœ í˜•ì˜ ê°€ì„¤ì— ëŒ€í•œ í¸í–¥ì„ ê°€ì§ˆ ë•Œ ì´ë¥¼ biasedë¼ê³  í•œë‹¤.
- eg) ë¹„ìŠ·í•œ sampleì€ ë¹„ìŠ·í•œ labelì´ì–´ì•¼ í•œë‹¤ë¼ëŠ” biasë¥¼ ê°€ì§ˆ ê²½ìš°
	- Aë¥¼ Bë³´ë‹¤ ì„ í˜¸í•˜ëŠ” í¸í–¥ì„ ê°€ì§

![2](/2.png)

- eg) ì˜¤ì»´ì˜ ë©´ë„ë‚  (Occam's razor)
: ë” ê°„ë‹¨í•˜ê³  ëª…ë£Œí•œ ê°€ì„¤ì„ ì„ íƒí•´ì•¼ í•œë‹¤.

No Free Lunch theorem (NFL) 
: ë” ì¢‹ì€ inductive biasëŠ” ì—†ë‹¤. (ê¸°ëŒ€ ì„±ëŠ¥ì€ ê°™ë‹¤)


pf)


{% raw %}
$$
\begin{aligned}&\text{let Hypothesis space } \chi \text{ must be discrete} \\ &P({\bf x}) : \text{prob. of samp } {\bf x} \\  &\text{II}(\cdot) : \text{1 if } \cdot \text{ is truth else 0} \\ &P(h \vert X, A_a) : \text{Xë¡œ í•™ìŠµí•œ Algorithm aê°€ ìƒì„±í•œ hì˜ í™•ë¥ } \\ &f : \text{objective function}\end{aligned}
$$
{% endraw %}


{% raw %}
$$
\begin{aligned}E_{ote}(A_a \vert X, f) =& \sum_h \sum_{x\in \chi-X} P({\bf x}) \cdot \text{II}(h({\bf x})-f({\bf x})) P(h \vert X, A_a)\\ =& \sum_{x\in \chi-X} P({\bf x}) \sum_h P(h \vert X, A_a) \sum_f \text{II}(h({\bf x})-f({\bf x})) \\  =& \sum_{x\in \chi-X} P({\bf x}) \sum_h P(h \vert X, A_a) \cdot \frac{1}{2} \cdot 2^{\vert \chi \vert} \\ =& 2^{\vert \chi \vert - 1} \sum_{x\in \chi-X} P({\bf x})\end{aligned}
$$
{% endraw %}

- $E_{ote}$ is invarient to algorithm
- Learning from NFL theorem : ì•Œê³ ë¦¬ì¦˜ì˜ ì¢‹ê³  ë‚˜ì¨ì— ëŒ€í•œ ë…¼ì˜ëŠ” ë¬´ì˜ë¯¸í•˜ë‹¤.
