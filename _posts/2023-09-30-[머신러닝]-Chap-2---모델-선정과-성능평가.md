
### Disclaimer


{: .prompt-info }


> üì£ Î≥∏ Ìè¨Ïä§Ìä∏Îäî Ï°∞Ïö∞ÏØîÌôîÏùò [Îã®Îã®Ìïú Î®∏Ïã†Îü¨Îãù](https://product.kyobobook.co.kr/detail/S000001916959) Ï±ÖÏùÑ ÏöîÏïΩ Ï†ïÎ¶¨Ìïú Í∏ÄÏûÖÎãàÎã§. 


# Chap 2. Model selection and Evaluation


## 2.1 Empirical error and Overfitting


error rate
: propotion of incorrectly classified samples $E = a/m$


Ï†ïÎ∞ÄÎèÑ
: 1-Error rate, $1-a/m$


Error
: difference beteween the output predicted by learner and ground-truth
LearnerÏôÄ Ground truthÍ∞í ÏÇ¨Ïù¥Ïùò Ï∞®Ïù¥


training error, empirical error
: training setÏóêÏÑú ÎßåÎì§Ïñ¥ÏßÑ error


generalization error
: testing setÏóêÏÑú ÎßåÎì§Ïñ¥ÏßÑ error


overfitting
: Í≥ºÏ†ÅÌï©, training dataÏùò ÌäπÏÑ±ÏùÑ Î™®Îì† Îç∞Ïù¥ÌÑ∞Ïùò ÏùºÎ∞òÏ†Å ÏÑ±ÏßàÎ°ú ÌïôÏäµÌïòÎäî Í≤É.


## 2.2 Evaluation method


### Hold-out

- Data setÏùò ÏùºÎ∂ÄÎ∂ÑÏùÑ ÎñºÏñ¥ÎÇ¥ÏÑú Ïù¥Î•º testing setÏúºÎ°ú ÏÇ¨Ïö©ÌïòÎäî Í≤É.
- Hold-out Ìï† Îïå ÎÇòÎàÑÏñ¥ÏßÄÎäî setÎì§Ïùò Îç∞Ïù¥ÌÑ∞ Î∂ÑÌè¨Í∞Ä Í∞ôÏïÑÏïº Ìï®.

### Cross validation

- kÍ∞úÎ°ú ÎÇòÎàå Í≤ΩÏö∞ k-fold cross validationÏù¥ÎùºÍ≥† Î∂àÎ¶º
- ÏÑúÎ°ú Í≤πÏπòÏßÄ ÏïäÎäî kÍ∞úÏùò setÏúºÎ°ú ÎÇòÎàÑÍ≥† Í∑∏ Ï§ë ÌïòÎÇòÎ•º testing setÏúºÎ°ú, ÎÇòÎ®∏ÏßÄÎ•º training setÏúºÎ°ú ÌïòÏó¨ kÎ≤àÏùò ÌõàÎ†®Í≥º ÌÖåÏä§Ìä∏Î•º Í±∞Ïπ®
- ÏµúÏ¢Ö Í∞íÏùÄ Í∞Å ÌÖåÏä§Ìä∏Ïùò Í≤∞Í≥ºÍ∞íÏùò ÌèâÍ∑†ÏúºÎ°ú Ìï®.
- Ïù¥Î•º pÎ≤à Î∞òÎ≥µÌïòÏó¨ pÏ∞® kÍ≤π ÍµêÏ∞®Í≤ÄÏ¶ù (eg 10Ï∞® 10Í≤π ÍµêÏ∞® Í≤ÄÏ¶ù)ÏùÑ ÏûêÏ£º ÏÇ¨Ïö©

### LOOCV(Leave-One-Out Cross Validation)

- \# of data samplesÍ∞Ä $m$Ïùº Îïå, $k=m$Ïù∏ cross validationÏùÑ leave-one-out cross validaionÏù¥Îùº Ìï®.
- data sample Ïàò ÎßåÌÅº training, testingÏùÑ ÏßÑÌñâÏãúÏºúÏïº Ìï®.

### Bootstrapping

- Initial data set DÏóêÏÑú Ï§ëÎ≥µÏùÑ ÌóàÏö©ÌïòÏó¨1Í∞úÏî© ÎΩëÏïÑ DÏùò ÌÅ¨Í∏∞ÎßåÌÅº ÎΩëÎäîÎã§.
- Ï¶â $D=\{1, 2, 3, 4\}$ÎùºÎ©¥, $D'=\{1,1,2,4\}$ Ïù∏ Í≤ÉÏù¥Îã§.
- ÏàòÌïôÏ†ÅÏúºÎ°ú ,

{% raw %}
$$
\lim\limits_{m\rightarrow \infty} \bigg(1-\frac{1}{m}\bigg)^m = \frac{1}{e}\approx 0.368
$$
{% endraw %}


- Ï¶â 36.8%Ïùò sampleÏùÄ $D'$Ïóê Îã¥Í∏∞ÏßÄ ÏïäÍ≤å Îê®. Ï¶â Ïù¥ ÏßëÌï©($D-D'$)ÏùÑ testing setÏúºÎ°ú ÌïòÍ≥† D'ÏùÑ training setÏúºÎ°ú ÌôúÏö©ÌïòÎäî Í≤ÉÏùÑ Out-of-Bag ÏòàÏ∏°Ïù¥Îùº Î∂ÄÎ¶Ñ.

### Parameter tuning


validation set
: ÌååÎùºÎØ∏ÌÑ∞ ÌäúÎãùÏùÑ ÏúÑÌï¥ testing setÏùÑ Í±∞ÏπòÍ∏∞ Ï†ÑÏóê ÎØ∏Î¶¨ Î™®Îç∏ ÌèâÍ∞Ä Î∞è ÏÑ†ÌÉù Í≥ºÏ†ïÏóêÏÑú Ïì∞Îäî ÌèâÍ∞Ä Îç∞Ïù¥ÌÑ∞ ÏßëÌï©


## 2.3 Î™®Îç∏ ÏÑ±Îä• Ï∏°Ï†ï

- ÌèâÍ∑†Ï†úÍ≥±Ïò§Ï∞® (MSE, mean squared error) :

{% raw %}
$$
E(f;D) = \frac{1}{m} \sum_{i=1}^m (f({\bf x_i})-y_i)^2
$$
{% endraw %}



ÌôïÎ•† Î∞ÄÎèÑ Ìï®ÏàòÎ°ú ÌëúÌòÑÌïòÎ©¥, 


{% raw %}
$$
E(f;D) = \int_{x\sim D} (f({\bf x})-y)^2 p({\bf x} ) d({\bf x})
$$
{% endraw %}


- Ïò§Ï∞®Ïú®, Ï†ïÌôïÎèÑ

{% raw %}
$$
E(f;D) = \frac{1}{m} \sum_{i=1}^m II(f({\bf x_i})\neq y_i)
$$
{% endraw %}



Ï†ïÌôïÎèÑÎäî 1-Ïò§Ï∞®Ïú®,


{% raw %}
$$
acc(f;D) = \frac{1}{m} \sum_{i=1}^m II(f({\bf x_i})= y_i)
$$
{% endraw %}



ÌôïÎ•†Î∞ÄÎèÑÌï®ÏàòÎ°ú ÎÇòÌÉÄÎÇ∏ Í≤ΩÏö∞ÏóêÎèÑ MSEÏóêÏÑú ÎÇòÌÉÄÎÇ∏ Í≤ÉÍ≥º ÎèôÏùºÌïòÍ≤å ÎÇòÌÉÄÎÇº Ïàò ÏûàÏùå.


### Confusion matrix

- True/False : Ïã§Ï†ú Í∞íÍ≥º ÏòàÏ∏° Í∞íÏù¥ ÏùºÏπòÌïòÎäîÏßÄ
- Positive/Negative : ÏòàÏ∏°Ìïú Í∞íÏù¥ ÏñëÏÑ±Ïù∏ÏßÄ ÏùåÏÑ±Ïù∏ÏßÄ

> Instruction :  
> 1. P, NÏùÑ Î®ºÏ†Ä ÏÉùÍ∞ÅÌïúÎã§. ex) FNÏù¥Î©¥, ÏùºÎã® NegativeÏù¥ÎØÄÎ°ú ÏòàÏ∏°ÏùÄ negativeÏûÑ.  
> 2. Ïù¥Ï†ú T, FÎ•º ÏÉùÍ∞ÅÌïúÎã§. FÏù¥ÎØÄÎ°ú, negativeÏôÄ Î∞òÎåÄÏù∏ positiveÍ∞Ä ground truthÏûÑÏùÑ Ïïå Ïàò ÏûàÏùå.

- Confusion matrix : TP, FN, FP, TNÏùÑ Í∞ÅÍ∞Å matrixÏóê ÎÇòÌÉÄÎÇ∏ Í≤É.

| Ïã§Ï†ú Í∞í : ÏòàÏ∏°Í∞í | ÏñëÏÑ± | ÏùåÏÑ± |
| ---------- | -- | -- |
| ÏñëÏÑ±         | TP | FN |
| ÏùåÏÑ±         | FP | TN |

- TP(true positive) : AÎ•º AÎùºÍ≥† ÏòàÏ∏°Ìïú Í≤É
- TN(true negative) : ~AÎ•º ~AÎùºÍ≥† ÏòàÏ∏°Ìïú Í≤É
- FP(false positive) : ~AÎ•º AÎùºÍ≥† ÏòàÏ∏°Ìïú Í≤É
- FN(false negative) : AÎ•º ~AÎùºÍ≥† ÏòàÏ∏°Ìïú Í≤É

Îã® binary classificationÏù¥ ÏïÑÎãå Í≤ΩÏö∞ÏóêÎèÑ confusion matrixÎ•º Ï†ÅÏö©Ìï† Ïàò ÏûàÏùå.


| ground truth : prediction | A | B  | C  | D  |
| ------------------------- | - | -- | -- | -- |
| A                         | 9 | 1  | 0  | 0  |
| B                         | 1 | 15 | 3  | 1  |
| C                         | 5 | 0  | 24 | 1  |
| D                         | 0 | 4  | 1  | 15 |

- Ï†ïÌôïÎèÑ(accuracy) in confusion matrix :

{% raw %}
$$
acc = \frac{TP+TN} {TP+TN+FP+FN}
$$
{% endraw %}


- MulticlassÏùò Í≤ΩÏö∞ diagonal(only true positive)ÎßåÏùÑ Î™®Îëê ÎçîÌï¥ mÏúºÎ°ú ÎÇòÎàà Í∞íÏùÑ ÏùòÎØ∏
- ÏúÑ ÏòàÏãúÏóêÏÑúÏùò Ï†ïÌôïÎèÑÎäî $(9+15+24+15)/80 = 0.78$

### Precision

- Ï†ïÎ∞ÄÎèÑ. positiveÎùºÍ≥† ÏòàÏ∏°Ìïú Í≤É Îì§ Ï§ë Ïã§Ï†úÎ°ú positiveÏù∏ Í≤ÉÎì§Ïùò ÎπàÎèÑ

{% raw %}
$$
prec = \frac{TP} {TP+FP}
$$
{% endraw %}



### Recall

- Ïû¨ÌòÑÏú®. ground truthÍ∞Ä positiveÏù∏ Í≤ÉÎì§ Ï§ë Î™®Îç∏Ïù¥ positiveÎùºÍ≥† ÏòàÏ∏°Ìïú Í≤ÉÏùò ÎπàÎèÑ

{% raw %}
$$
rec = \frac{TP} {TP + FN}
$$
{% endraw %}



### P-R Curve, AUPRC


![](/assets/img/2023-09-30-[Î®∏Ïã†Îü¨Îãù]-Chap-2---Î™®Îç∏-ÏÑ†Ï†ïÍ≥º-ÏÑ±Îä•ÌèâÍ∞Ä.md/0.png)

- PrecisionÍ≥º RecallÏùÄ ÏùºÏ¢ÖÏùò tradeoffÍ∞Ä Ï°¥Ïû¨Ìï®.
- PrecisionÏùÑ yÏ∂ïÏóê, RecallÏùÑ xÏ∂ïÏóê ÎëêÍ≥† Í∑∏ÎûòÌîÑÎ•º ÎßåÎì† Í≤ÉÏù¥ P-R curveÏûÑ.
- thresholdÎ•º Ï†êÏ∞® Î∞îÍæ∏Ïñ¥ Í∞ÄÎ©∞, Í∞ÅÍ∞ÅÏùò confusion matrixÎ°úÎ∂ÄÌÑ∞ (P, R)ÏùÑ Íµ¨ÌïòÎäî Î∞©Î≤ïÏùÑ Ïù¥Ïö©.
- BEP (Break-Even Point)
: $y=x$ÏôÄ PR curveÍ∞Ä ÎßåÎÇòÎäî Ï†ê, PR curve Í∞Ä intersectÌï† Í≤ΩÏö∞ ÌèâÍ∞ÄÌïòÎäî Î∞©Î≤ïÏ§ë ÌïòÎÇò.
- ÏùºÎ∞òÏ†ÅÏúºÎ°ú Area Under (PR) Curve (AUPRC)Í∞Ä ÌÅ∞ classifierÎ•º Ï¢ãÏùÄ classifierÎùºÍ≥† Î¥Ñ.
- Í∑∏Îü¨ÎÇò PR curve Í∞ÑÏóê crossÍ∞Ä ÏùºÏñ¥ÎÇòÎäî Í≤ΩÏö∞ÏóêÎäî AUCÎßåÏúºÎ°ú ÎπÑÍµêÌïòÍ∏∞Í∞Ä Ïï†Îß§Ìï®.
- P, RÏùò Ï†ïÏùòÏóê ÏùòÌï¥ÏÑú AUC=1Ïù∏, 1x1Ïùò square ÌòïÌÉúÎ°ú ÎÇòÌÉÄÎÇòÎäî Í≤ÉÏù¥ Í∞ÄÏû• Ïù¥ÏÉÅÏ†ÅÏûÑ.
- $(1, 1)$Ïóê Í∞ÄÏû• Í∞ÄÍπåÏö¥ thresholdÎ•º Í≥†Î•¥Îäî Í≤ÉÏù¥ Ïù¥ÏÉÅÏ†Å.

### F1-score

- Ï†ïÎ∞ÄÎèÑ, Ïû¨ÌòÑÏú® Îëê Í∞íÏùÑ Ï°∞Ìôî ÌèâÍ∑†ÌïòÏó¨ ÌïòÎÇòÏùò ÏàòÏπòÎ°ú ÎÇòÌÉÄÎÇ∏ ÏßÄÌëú
- Ï†ïÎ∞ÄÎèÑ($P$), Ïû¨ÌòÑÏú®($R$)Îùº Ìï† Îïå

{% raw %}
$$
\text{F1 score = } \frac{2PR}{P+R}
$$
{% endraw %}



### F-beta-score

- F1-scoreÎäî Precision, RecallÏùò Í∞ÄÏ§ëÏπòÎ•º ÎèôÏùºÌïòÍ≤å Îëî Í≤ΩÏö∞Ïù¥Î©∞, Ïã§Ï†ú ÏÉÅÌô©ÏóêÏÑú Ïñ¥Îäê ÌäπÏ†ï Í∞íÏóê Í∞ÄÏ§ëÏπòÎ•º ÎëêÏñ¥Ïïº Ìï† Í≤ΩÏö∞ F-beta scoreÎ•º ÏÇ¨Ïö©Ìï®. $\beta>1$Ïù∏ Í≤ΩÏö∞Îäî $R$Ïùò ÏòÅÌñ•Ïù¥ Îçî ÌÅ¨Î©∞, $\beta<1$Ïùò Í≤ΩÏö∞Îäî $P$Ïùò ÏòÅÌñ•Ïù¥ Îçî ÌÅº.

{% raw %}
$$
F_\beta = \frac{(1+\beta^2)\times P \times R} {(\beta^2\times P) + R}
$$
{% endraw %}



### (Macro, Micro) P, R, F1-score

- Ïó¨Îü¨ Í∞úÏùò ÌòºÎèô ÌñâÎ†¨ÏùÑ ÏñªÍ≤å Îê† Í≤ΩÏö∞, Í∞Å Confusion matrixÏóêÏÑú ÏñªÏùÄ P, R, F1Í∞íÎì§ÏùÑ Í≥ÑÏÇ∞ÌïòÍ≥† Ïù¥ Í∞íÎì§ÏùÑ ÌèâÍ∑†Ìïú Í≤ÉÏù¥ macro-P, macro-R, macro-F1 Ïù¥Îùº ÌïúÎã§.
- Î∞òÎåÄÎ°ú Í∞ÅÍ∞ÅÏùò ÌòºÎèôÌñâÎ†¨Ïù¥ ÎåÄÏùëÌïòÎäî ÏõêÏÜåÎì§Ïóê ÎåÄÌïú TP, FP, TN, FNÎì§Ïùò ÌèâÍ∑†Í∞íÏùÑ Íµ¨ÌïòÍ≥† Ïù¥Î•º Ïù¥Ïö©ÌïòÏó¨ P, R, F1Í∞íÏùÑ Í≥ÑÏÇ∞ÌïòÎ©¥ Ïù¥Î•º micro-P, micro-R, micro-F1Ïù¥Îùº ÌïúÎã§.

### ROC, AUC

- thresholdÎ•º Ï†ïÌïòÍ∏∞ ÏúÑÌïú ÎèÑÍµ¨ : ROC

{% raw %}
$$
TPR = \frac{TP}{TP+FN}
$$
{% endraw %}



{% raw %}
$$
FPR = \frac{FP}{TN+FP}
$$
{% endraw %}


- True Positive Rate (TPR)
: ground truthÍ∞Ä TrueÏù∏ Í≤É Îì§ Ï§ëÏóêÏÑú classifierÍ∞Ä Ïã§Ï†úÎ°ú TrueÎùºÍ≥† Ìïú Í≤ÉÏùò ÎπÑÏú® (Recall)
- False Postive Rate (FPR)
: ground truthÍ∞Ä falseÏù∏ Í≤É Îì§ Ï§ëÏóêÏÑú classifierÍ∞Ä Ïã§Ï†úÎ°ú FalseÎùºÍ≥† Ìïú Í≤ÉÏùò ÎπÑÏú®
- TPRÏùÑ yÏ∂ï, FPRÏùÑ xÏ∂ïÏóê ÎÜìÍ≥† Í∑∏ÎûòÌîÑÎ°ú ÌëúÌòÑÌïò Í≤ÉÏù¥ RPC Í∑∏ÎûòÌîÑ
- y=xÎäî ÎûúÎç§ ÏòàÏ∏° Î™®Îç∏Ïùò Í≤ΩÏö∞Ïùº Í≤ÉÏù¥Î©∞, AUCÍ∞Ä ÌÅ¥ ÏàòÎ°ù Îçî Ï¢ãÏùÄ Î™®Îç∏ÏûÑ.
- AUCÎäî ÏàúÏÑú ÏòàÏ∏° ÌíàÏßà, Ï¶â ÏàúÏÑúÏùò lossÏôÄ ÌÅ∞ Í¥ÄÎ†®Ïù¥ ÏûàÏùå $AUC=1-l_{rank}$

### Cost-sensitive error rate, cost curve

- FN, FPÍ∞Ä ÏÑúÎ°ú Îã§Î•∏ costÎ•º Í∞ÄÏßÄÎäî Í≤ΩÏö∞ unequal costÎùºÎäî Í∞úÎÖê Ï†ÅÏö©Í∞ÄÎä•.
- ÎèÑÎ©îÏù∏ ÏßÄÏãùÏóê Í∏∞Î∞òÏúºÎ°ú cost matrixÎ•º ÏÉùÏÑ±

| Ïã§Ï†ú Í∞í : ÏòàÏ∏°Í∞í | ÏñëÏÑ±          | ÏùåÏÑ±          |
| ---------- | ----------- | ----------- |
| ÏñëÏÑ±         | 0           | $cost_{01}$ |
| ÏùåÏÑ±         | $cost_{10}$ | 0           |

- ÎπÑÍ∑†Îì± ÎπÑÏö©ÏóêÏÑúÎäî Ïò§Ï∞® ÌöüÏàòÏùò ÏµúÏÜåÌôîÍ∞Ä ÏïÑÎãå ÎπÑÏö©(total cost)Ïùò ÏµúÏÜåÌôîÎ•º Î™©Ï†ÅÏúºÎ°ú ÌïòÎØÄÎ°ú, cost-sensitive error rateÎäî Îã§ÏùåÍ≥º Í∞ôÏù¥ Ï†ïÏùòÎêúÎã§.

{% raw %}
$$
\begin{aligned}E(f;D;cost) = \frac{1}{m}\bigg(\sum_{{\bf x_i}\in D^+}\text{II}(f({\bf x_i})\neq y_i) \times cost_{01}+ \\ \sum_{{\bf x_i}\in D^-}\text{II}(f({\bf x_i})\neq y_i) \times cost_{10}\bigg)\end{aligned}
$$
{% endraw %}



### Cost curve

- x axis of cost curves is probability cost of positive class

{% raw %}
$$
P(+)cost = \frac{p \times cost_{01}}{p\times cost_{01}+(1-p)\times cost_{10}}
$$
{% endraw %}


- $p\in [0,1]$ Îäî sampleÏù¥ positiveÏùº ÌôïÎ•†. $p=0$Ïùº Îïå $P(+)cost =0$Ïù¥Í≥†, 1Ïùº Îïå 1Ïù¥Îãà ÎπÑÏö©Í∞ÄÏ§ë(ÎπÑÏö©ÏùÑ Í≥†Î†§Ìïú) sampleÏù¥ positiveÌï† ÌôïÎ•† ÏùÑ xÏ∂ïÏóê ÎÜìÏïòÎã§Í≥† Î≥º Ïàò ÏûàÏùå.
- y axis is normalized cost which takes values from [0,1]

{% raw %}
$$
cost_{norm} = \frac{FNR \times p \times cost_{01} + FPR \times (1-p)\times cost_{10}} {p\times cost_{01} + (1-p)\times cost_{10}}
$$
{% endraw %}


- ÎÇ¥Í∞Ä pÏùò ÌôïÎ•†Ïóê Îî∞Îùº Î∂ÑÎ•òÌïú Í≤ÉÏù¥ Îã§ ÌãÄÎ†∏ÏùÑ ÎïåÏùò Ï†ÑÏ≤¥ costÎ•º Î∂ÑÎ™®Î°ú, pÏùò ÌôïÎ•†Î°ú Î∂ÑÎ•òÌïòÎÇò FNR, FPRÏùÑ Í≥†Î†§ÌïòÏó¨ Î∂ÑÎ•òÌñàÏùÑ ÎïåÏùò costÎ•º Î∂ÑÏûêÎ°ú ÌïòÎäîÎäêÎÇå? ÏµúÏïÖÏùò Í≤ΩÏö∞Ïùò ÏµúÎåÄ cost Î∂ÑÏóê ÌòÑÏû¨ ÌïôÏäµÍ∏∞Ïùò costÎ•º normalized costÎùºÍ≥† ÌïòÎäî ÎìØ
- $FNR = 1-TPR$
- $p=0$Ïùº ÎïåÏùò $cost_{norm}=FNR$, $p=1$Ïùº ÎïåÏùò $cost_{norm} = FPR$Ïù¥ÎØÄÎ°ú, cost-plane ÏÉÅÏóê Í∞ÅÍ∞ÅÏùò ROC curve Ïùò Ï†êÎì§Ïóê ÎåÄÌïú $(0,FPR), (1,1-TPR)$ÏùÑ ÏûáÎäî ÏßÅÏÑ†Îì§ÏùÑ Î™®Îëê plot ÌïòÏó¨ ÏßÅÏÑ†Îì§ ÏïÑÎûòÏùò Î©¥Ï†ÅÏù¥ Í∏∞ÎåÄ Ï¥ù ÎπÑÏö© (expected total cost)Í∞Ä Îê† Í≤É.

![](/assets/img/2023-09-30-[Î®∏Ïã†Îü¨Îãù]-Chap-2---Î™®Îç∏-ÏÑ†Ï†ïÍ≥º-ÏÑ±Îä•ÌèâÍ∞Ä.md/1.png)


## 2.4 ÌïôÏäµÍ∏∞ ÏÑ±Îä• ÎπÑÍµê Í≤ÄÏ¶ù


### Ïù¥Ìï≠ Í≤ÄÏ†ï (binomial test)

- $\epsilon\leq\epsilon_0$ ÎùºÎäî Í∞ÄÏÑ§ÏùÑ ÏÑ∏Ïö∞Í≥† $\alpha=0.05$ Îì±Ïùò significance ÌïòÏóêÏÑú Í∞ÄÏÑ§ÏùÑ Í∏∞Í∞ÅÌï† Ïàò ÏûàÎäîÏßÄ ÌôïÏù∏.
- Ìï¥Îãπ Ïò§Ï∞®Ïú®($\epsilon_0$)Î≥¥Îã§ Ïò§Ï∞®Ïú®Ïù¥ ÌÅ¥ ÌôïÎ•†Í≥º $\alpha$ÏôÄ ÎπÑÍµê

### t-test (Ïó¨Îü¨ test-setÏóê ÎåÄÌïú Ïò§Ï∞®Ïú®ÏùÑ Í≤ÄÏ†ïÌï† Îïå)

- kÍ∞úÏùò ÌÖåÏä§Ìä∏ Ïò§Ï∞®Ïú® $\hat\epsilon_1, \hat\epsilon_2\cdots,\hat\epsilon_k$ÏùÑ ÏñªÎäîÎã§Î©¥, Í∞ÅÍ∞ÅÏùò ÌèâÍ∑† Ïò§Ï∞®Ïú® $\mu$ÏôÄ Î∂ÑÏÇ∞ $\sigma^2$ÏùÑ ÏñªÏùÑ Ïàò ÏûàÏùå.
- Ïù¥ Îïå Í∞ÅÍ∞ÅÏùò ÌÖåÏä§Ìä∏ Ïò§Ï∞®Ïú®Ïù¥ ÎèÖÎ¶ΩÌëúÎ≥∏Ïù¥ÎùºÎ©¥ Í≤ÄÏ†ïÌÜµÍ≥ÑÎüâÏùÄ Îã§ÏùåÍ≥º Í∞ôÏùå.

{% raw %}
$$
\tau_t = \frac{\sqrt k (\mu-\epsilon_0)}{\sigma}\sim t(k-1)
$$
{% endraw %}


- $\alpha$Ïùò significance ÌïòÏóêÏÑú, $\mu=\epsilon_0$Ïùò Í∑ÄÎ¨¥Í∞ÄÏÑ§ÏùÑ Í∏∞Í∞ÅÌï† Ïàò ÏûàÎäî Í∏∞Í∞ÅÏó≠ÏùÄ $(-\infty,t_{-\alpha/2}]$, $[t_{-\alpha/2}, \infty)$ Ïù¥Îã§.

### ÍµêÏ∞® Í≤ÄÏ¶ù t-test

- Îëê ÌïôÏäµÍ∏∞Ïùò Ïò§Ï∞®Ïú®Ïù¥ Í∞ôÎã§Îäî Í∑ÄÎ¨¥Í∞ÄÏÑ§ÏùÑ ÏÑ∏Ïö∞Í≥†, ÎèôÏùºÌïú test-setÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ ÎÇòÏò® Îëê ÌïôÏäµÍ∏∞Ïùò Ïò§Ï∞®Ïú®Ïùò Ï∞®($\Delta_i$)Î•º t-test ÌïúÎã§.

{% raw %}
$$
\tau_t = \vert\frac{\sqrt{k}\mu}{\sigma}\vert
$$
{% endraw %}


- Í∞ôÏùÄ Î∞©Î≤ïÏúºÎ°ú Í∏∞Í∞ÅÏó≠ÏùÑ ÏÑ∏Ïõå t-test ÌïòÎ©¥ Îê®.
- Test ÎÇ¥Ïùò Ï§ëÎ≥µ Îç∞Ïù¥ÌÑ∞ Îì±Ïù¥ Ï°¥Ïû¨Ìï† Í≤ΩÏö∞ Ïò§Ï∞®Ïú®Ïù¥ ÎèÖÎ¶ΩÏ†ÅÏù¥ÏßÄ ÏïäÏúºÎØÄÎ°ú, 5 x 2 cross-validation Îì±ÏùÑ ÌôúÏö©.

### 5 x 2 cross-validation

- testing setÏùÑ 5Í∞úÎ°ú ÎÇòÎàÑÍ≥†, Í∞ÅÍ∞ÅÏùò setÏùÑ Îòê 2Í∞úÎ°ú ÎÇòÎàÑÏñ¥ 2-fold cross-validationÏùÑ ÏàòÌñâÌïúÎã§. 2Í∞úÏùò Ïò§Ï∞®Ïú® Ï∞®Ïù¥Í∞íÏù¥ ÎèÑÏ∂ú($\Delta_i^1, \Delta_i^2 \text{ for i=1}\cdots 5$)
- ÎπÑÎèÖÎ¶ΩÏÑ±Ïùò ÏôÑÌôîÎ•º ÏúÑÌï¥(?) i=1ÏùºÎïåÏùò Ïò§Ï∞®Ïú®Ïùò ÌèâÍ∑†ÏùÑ $\mu$Î°ú ÌôúÏö©
- Î∂ÑÏÇ∞ÏùÄ Í∞ÅÍ∞ÅÏùò 5Í∞úÏóê ÎåÄÌïú Î∂ÑÏÇ∞Ïùò Ìï©ÏùÑ Ïù¥Ïö©.

{% raw %}
$$
\sigma_i^2 = \bigg(\Delta_i^1-\frac{\Delta_i^1+\Delta_i^2}{2}\bigg)^2+\bigg(\Delta_i^2-\frac{\Delta_i^1+\Delta_i^2}{2}\bigg)^2
$$
{% endraw %}


- Ïù¥Î†áÍ≤å Í≥ÑÏÇ∞Îêú TÍ≤ÄÏ†ïÌÜµÍ≥ÑÎüâÏùÄ

{% raw %}
$$
\tau_t = \frac{\mu}{\sqrt{0.2 \sum_{i=1}^5 \sigma_i^2}}
$$
{% endraw %}



### Îß•ÎãàÎßà(McNemar) test

- contingency table of two learners

| Alg.A : Alg.B | Correct  | Incorrect |
| ------------- | -------- | --------- |
| Correct       | $e_{00}$ | $e_{01}$  |
| Incorrect     | $e_{10}$ | $e_{11}$  |

- Îëê ÌïôÏäµÍ∏∞Ïùò ÏÑ±Îä•Ïù¥ ÎèôÏùºÌïòÎã§Îäî Í≤ÉÏùÄ $e_{01}=e_{10}$ÏûÑ. Ï¶â ÏÑúÎ°ú Îã§Î•¥Í≤å ÌåêÎ≥ÑÌïú Í∞úÏàòÏùò Ï∞®Í∞Ä Ï†ïÍ∑úÎ∂ÑÌè¨Î•º Ïù¥Î£∞ Í≤ÉÏù¥ÎØÄÎ°ú,

{% raw %}
$$
\tau_{\chi^2} = \frac{(\vert e_{01}-e_{10}\vert -1)^2}{e_{01}+e_{10}} \sim \chi^2_1
$$
{% endraw %}


- ÏúÑ Í≤ÄÏ†ïÌÜµÍ≥ÑÎüâÏùÄ ÏûêÏú†ÎèÑÍ∞Ä 1Ïù∏ Ïπ¥Ïù¥Ï†úÍ≥± Î∂ÑÌè¨Î•º Îî∞Î•¥Í≤å Îê† Í≤ÉÏù¥Îã§

### ÌîÑÎ¶¨ÎìúÎ®º(Friedman) test

- Îã§ÏàòÏùò ÏïåÍ≥†Î¶¨Ï¶òÏùò RankingÏùÑ Îß§Í∏∏ Îïå ÏÇ¨Ïö©ÌïòÎäî Í≤ÄÏ†ïÎ∞©Î≤ï
- ÏïûÏÑ† Í≤ÄÏ¶ùÎ≤ïÎì§ÏùÑ Ïù¥Ïö©ÌïòÏó¨ Í∞Å data set, Í∞Å algorithmÎ≥Ñ ÌÖåÏä§Ìä∏ Í≤∞Í≥ºÎ•º ÏñªÍ≥† ÏàúÏúÑÎ•º Îß§ÍπÄ (Îã®, ÏÑ±Îä•Í∞íÏù¥ Î™®Îëê Í∞ôÎã§Î©¥ ÌèâÍ∑† Í∞íÏùÑ Îß§ÍπÄ)
- Í∞ÅÍ∞ÅÏùò ÏïåÍ≥†Î¶¨Ï¶òÏùò ÌèâÍ∑† Îì±ÏàòÏóê ÎåÄÌïú ÌèâÍ∑†Í≥º Î∂ÑÏÇ∞ÏùÑ Íµ¨Ìï† Ïàò ÏûàÏúºÎ©∞, NÍ∞úÏùò data set, kÍ∞úÏùò algorithmÏóê ÎåÄÌï¥

{% raw %}
$$
\tau_{\chi^2} = \frac{k-1}{k} \cdot \frac{12N}{k^2-1} \sum_{i=1}^k \bigg(r_i-\frac{k+1}{2}\bigg)^2 \sim \chi^2_{k-1}
$$
{% endraw %}


- ÏúÑ Í≤ÄÏ†ï ÌÜµÍ≥ÑÎüâÏùÄ ÏûêÏú†ÎèÑÍ∞Ä k-1Ïù∏ Ïπ¥Ïù¥Ï†úÍ≥± Î∂ÑÌè¨Î•º ÏÇ¨Ïö©.
- ÌîÑÎ¶¨ÎìúÎ®º Í≤ÄÏ†ïÏù¥ Î≥¥ÏàòÏ†ÅÏù¥ÎØÄÎ°ú(Ï∞®Ïù¥Í∞Ä ÏóÜÎã§Í≥† ÌåêÎ≥ÑÌï† Í∞ÄÎä•ÏÑ±Ïù¥ ÌÅº) Í∞úÏÑ†Îêú ÌîÑÎ¶¨ÎìúÎ®º Í≤ÄÏ†ïÏùÑ ÏÇ¨Ïö©.

{% raw %}
$$
\tau_F = \frac{(N-1)\tau_{\chi^2}}{N(k-1)-\tau_{\chi^2}}\sim F_{k-1, (k-1)(N-1)}
$$
{% endraw %}


- ÏúÑ Í≤ÄÏ†ï ÌÜµÍ≥ÑÎüâÏùÄ ÏûêÏú†ÎèÑ $k-1$, $(k-1)(N-1)$ÏùÑ Îî∞Î•¥Îäî FÎ∂ÑÌè¨ÏûÑ.

### ÎÑ§Î©îÎãà ÏÇ¨ÌõÑ Í≤ÄÏ†ï(Nemenyi post-hoc test)


{% raw %}
$$
CD = q_\alpha \sqrt{\frac{k(k+1)}{6N}}
$$
{% endraw %}


- q : Tukey Î∂ÑÌè¨
- CD Í∞íÏùÑ Í≥ÑÏÇ∞ÌïòÏó¨ÏÑú Îëê ÏïåÍ≥†Î¶¨Ï¶ò ÏÑ±Îä•Ïùò ÌèâÍ∑†Í∞í Ï∞®Ïù¥Í∞Ä Ìï¥Îãπ ÏàòÏπò Ïù¥ÏÉÅÏùº Í≤ΩÏö∞ Îëê ÏïåÍ≥†Î¶¨Ï¶òÏùÄ Ï∞®Ïù¥Í∞Ä ÏûàÎã§Í≥† Î≥º Ïàò ÏûàÏùå.

## 2.5 Ìé∏Ìñ•Í≥º Î∂ÑÏÇ∞

- $\bf x$ : ÌÖåÏä§Ìä∏ ÏÉòÌîå
- $y_D$ : label
- $y$ : ground-truth (Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ Í∞í)
- $f({\bf x}, D)$ : Î™®Îç∏Ïùò ÏòàÏ∏°Í∞í
- ÏïåÍ≥†Î¶¨Ï¶òÏùò Í∏∞ÎåÄ ÏòàÏ∏°Í∞í (ÏòàÏ∏°Í∞íÏùò ÌèâÍ∑† or Í∏∞ÎåìÍ∞í)

{% raw %}
$$
\bar f({\bf x}) = E_D[f({\bf x}, D)]
$$
{% endraw %}


- Î∂ÑÏÇ∞

{% raw %}
$$
var({\bf x}) = E_D[(f({\bf x}, D)-\bar f({\bf x}))^2]
$$
{% endraw %}


- ÎÖ∏Ïù¥Ï¶à

{% raw %}
$$
\epsilon^2 = E_D[(y_D-y)^2]
$$
{% endraw %}


- Ìé∏Ìñ•(Í∏∞ÎåÄ Í≤∞Í¥èÍ∞íÍ≥º Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞Ïùò Ï∞®Ïù¥)

{% raw %}
$$
bias^2({\bf x}) = (\bar f({\bf x})-y)^2
$$
{% endraw %}


- ÏùºÎ∞ò Ïò§Ï∞®(Total error)Í∞Ä Ìé∏Ìñ•, Î∂ÑÏÇ∞, ÎÖ∏Ïù¥Ï¶àÏùò Ìï©ÏúºÎ°ú Î∂ÑÌï¥Îê† Ïàò ÏûàÏùå.

{% raw %}
$$
\begin{aligned} Error(f;D) =& E_D [(f({\bf x};D)-y_D)^2]\\ =& E_D[(f({\bf x};D)-\bar f({\bf x})+\bar f({\bf x})-y_D)^2]\\ =& var({\bf x}) + E_D[(\bar f({\bf x})-y_D)^2]\\ &+ E_D[2(f({\bf x};D)-\bar f({\bf x}))(\bar f({\bf x})-y_D)] \\ =& var({\bf x}) + E_D[(\bar f({\bf x})-y+y-y_D)^2] \\ =& var({\bf x})+((\bar f({\bf x})-y)^2+E_D[(y_D-y)^2]\end{aligned}
$$
{% endraw %}



{% raw %}
$$
\therefore Error(f;D) = bias^2({\bf x})+ var({\bf x})+\epsilon^2
$$
{% endraw %}


<script>
  window.MathJax = {
    tex: {
      macros: {
        R: "\\\\mathbb{R}",
        N: "\\\\mathbb{N}",
        Z: "\\\\mathbb{Z}",
        Q: "\\\\mathbb{Q}",
        C: "\\\\mathbb{C}",
        proj: "\\\\operatorname{proj}",
        rank: "\\\\operatorname{rank}",
        im: "\\\\operatorname{im}",
        dom: "\\\\operatorname{dom}",
        codom: "\\\\operatorname{codom}",
        argmax: "\\\\operatorname*{arg\\,max}",
        argmin: "\\\\operatorname*{arg\\,min}",
        "\\{": "\\\\lbrace",
        "\\}": "\\\\rbrace",
        sub: "\\\\subset",
        sup: "\\\\supset",
        sube: "\\\\subseteq",
        supe: "\\\\supseteq"
      },
      tags: "ams",
      strict: false, 
      inlineMath: [["$", "$"], ["\\\\(", "\\\\)"]],
      displayMath: [["$$", "$$"], ["\\\\[", "\\\\]"]]
    },
    options: {
      skipHtmlTags: ["script", "noscript", "style", "textarea", "pre"]
    }
  };
</script>
<script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
