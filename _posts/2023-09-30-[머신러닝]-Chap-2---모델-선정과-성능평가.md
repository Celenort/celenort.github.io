---
layout: post
title: "[ë¨¸ì‹ ëŸ¬ë‹] Chap 2 - ëª¨ë¸ ì„ ì •ê³¼ ì„±ëŠ¥í‰ê°€"
date: 2023-09-30
draft: false
published: true
pin: false
image:
  path: "/assets/img/2023-09-30-[ë¨¸ì‹ ëŸ¬ë‹]-Chap-2---ëª¨ë¸-ì„ ì •ê³¼-ì„±ëŠ¥í‰ê°€/0.png"
  alt: "[ë¨¸ì‹ ëŸ¬ë‹] Chap 2 - ëª¨ë¸ ì„ ì •ê³¼ ì„±ëŠ¥í‰ê°€"
description: "ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì„ ì •ê³¼ ì„±ëŠ¥ í‰ê°€ì— ê´€í•œ ë‚´ìš©ìœ¼ë¡œ, Out-of-Bag ì˜ˆì¸¡, íŒŒë¼ë¯¸í„° íŠœë‹, ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •(MSE, ì •í™•ë„, ì •ë°€ë„, ì¬í˜„ìœ¨ ë“±), í˜¼ë™ í–‰ë ¬, ROC ê³¡ì„ , ë¹„ìš© ë¯¼ê° ì˜¤ë¥˜ìœ¨, ë‹¤ì–‘í•œ í†µê³„ ê²€ì • ë°©ë²•(ì´í•­ ê²€ì •, t-test, ë§¥ë‹ˆë§ˆ ê²€ì • ë“±), ê·¸ë¦¬ê³  í¸í–¥-ë¶„ì‚° ë¶„í•´ì— ëŒ€í•´ ì„¤ëª…í•˜ê³  ìˆìŠµë‹ˆë‹¤."
tags: ["Cross-validation", "Confusion matrix", "PR curve", "ROC curve"]
categories: ["Lecture", "Machine Learning"]
math: true
---


### Disclaimer


{: .prompt-info }


> ğŸ“£ ë³¸ í¬ìŠ¤íŠ¸ëŠ” ì¡°ìš°ì¯”í™”ì˜ [ë‹¨ë‹¨í•œ ë¨¸ì‹ ëŸ¬ë‹](https://product.kyobobook.co.kr/detail/S000001916959) ì±…ì„ ìš”ì•½ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤. 


# Chap 2. Model selection and Evaluation


## 2.1 Empirical error and Overfitting


error rate
: propotion of incorrectly classified samples $E = a/m$


ì •ë°€ë„
: 1-Error rate, $1-a/m$


Error
: difference beteween the output predicted by learner and ground-truth
Learnerì™€ Ground truthê°’ ì‚¬ì´ì˜ ì°¨ì´


training error, empirical error
: training setì—ì„œ ë§Œë“¤ì–´ì§„ error


generalization error
: testing setì—ì„œ ë§Œë“¤ì–´ì§„ error


overfitting
: ê³¼ì í•©, training dataì˜ íŠ¹ì„±ì„ ëª¨ë“  ë°ì´í„°ì˜ ì¼ë°˜ì  ì„±ì§ˆë¡œ í•™ìŠµí•˜ëŠ” ê²ƒ.


## 2.2 Evaluation method


### Hold-out

- Data setì˜ ì¼ë¶€ë¶„ì„ ë–¼ì–´ë‚´ì„œ ì´ë¥¼ testing setìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒ.
- Hold-out í•  ë•Œ ë‚˜ëˆ„ì–´ì§€ëŠ” setë“¤ì˜ ë°ì´í„° ë¶„í¬ê°€ ê°™ì•„ì•¼ í•¨.

### Cross validation

- kê°œë¡œ ë‚˜ëˆŒ ê²½ìš° k-fold cross validationì´ë¼ê³  ë¶ˆë¦¼
- ì„œë¡œ ê²¹ì¹˜ì§€ ì•ŠëŠ” kê°œì˜ setìœ¼ë¡œ ë‚˜ëˆ„ê³  ê·¸ ì¤‘ í•˜ë‚˜ë¥¼ testing setìœ¼ë¡œ, ë‚˜ë¨¸ì§€ë¥¼ training setìœ¼ë¡œ í•˜ì—¬ kë²ˆì˜ í›ˆë ¨ê³¼ í…ŒìŠ¤íŠ¸ë¥¼ ê±°ì¹¨
- ìµœì¢… ê°’ì€ ê° í…ŒìŠ¤íŠ¸ì˜ ê²°ê³¼ê°’ì˜ í‰ê· ìœ¼ë¡œ í•¨.
- ì´ë¥¼ pë²ˆ ë°˜ë³µí•˜ì—¬ pì°¨ kê²¹ êµì°¨ê²€ì¦ (eg 10ì°¨ 10ê²¹ êµì°¨ ê²€ì¦)ì„ ìì£¼ ì‚¬ìš©

### LOOCV(Leave-One-Out Cross Validation)

- \# of data samplesê°€ $m$ì¼ ë•Œ, $k=m$ì¸ cross validationì„ leave-one-out cross validaionì´ë¼ í•¨.
- data sample ìˆ˜ ë§Œí¼ training, testingì„ ì§„í–‰ì‹œì¼œì•¼ í•¨.

### Bootstrapping

- Initial data set Dì—ì„œ ì¤‘ë³µì„ í—ˆìš©í•˜ì—¬1ê°œì”© ë½‘ì•„ Dì˜ í¬ê¸°ë§Œí¼ ë½‘ëŠ”ë‹¤.
- ì¦‰ $D=\{1, 2, 3, 4\}$ë¼ë©´, $D'=\{1,1,2,4\}$ ì¸ ê²ƒì´ë‹¤.
- ìˆ˜í•™ì ìœ¼ë¡œ ,

{% raw %}
$$
\lim\limits_{m\rightarrow \infty} \bigg(1-\frac{1}{m}\bigg)^m = \frac{1}{e}\approx 0.368
$$
{% endraw %}


- ì¦‰ 36.8%ì˜ sampleì€ $D'$ì— ë‹´ê¸°ì§€ ì•Šê²Œ ë¨. ì¦‰ ì´ ì§‘í•©($D-D'$)ì„ testing setìœ¼ë¡œ í•˜ê³  D'ì„ training setìœ¼ë¡œ í™œìš©í•˜ëŠ” ê²ƒì„ Out-of-Bag ì˜ˆì¸¡ì´ë¼ ë¶€ë¦„.

### Parameter tuning


validation set
: íŒŒë¼ë¯¸í„° íŠœë‹ì„ ìœ„í•´ testing setì„ ê±°ì¹˜ê¸° ì „ì— ë¯¸ë¦¬ ëª¨ë¸ í‰ê°€ ë° ì„ íƒ ê³¼ì •ì—ì„œ ì“°ëŠ” í‰ê°€ ë°ì´í„° ì§‘í•©


## 2.3 ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •

- í‰ê· ì œê³±ì˜¤ì°¨ (MSE, mean squared error) :

{% raw %}
$$
E(f;D) = \frac{1}{m} \sum_{i=1}^m (f({\bf x_i})-y_i)^2
$$
{% endraw %}



í™•ë¥  ë°€ë„ í•¨ìˆ˜ë¡œ í‘œí˜„í•˜ë©´, 


{% raw %}
$$
E(f;D) = \int_{x\sim D} (f({\bf x})-y)^2 p({\bf x} ) d({\bf x})
$$
{% endraw %}


- ì˜¤ì°¨ìœ¨, ì •í™•ë„

{% raw %}
$$
E(f;D) = \frac{1}{m} \sum_{i=1}^m II(f({\bf x_i})\neq y_i)
$$
{% endraw %}



ì •í™•ë„ëŠ” 1-ì˜¤ì°¨ìœ¨,


{% raw %}
$$
acc(f;D) = \frac{1}{m} \sum_{i=1}^m II(f({\bf x_i})= y_i)
$$
{% endraw %}



í™•ë¥ ë°€ë„í•¨ìˆ˜ë¡œ ë‚˜íƒ€ë‚¸ ê²½ìš°ì—ë„ MSEì—ì„œ ë‚˜íƒ€ë‚¸ ê²ƒê³¼ ë™ì¼í•˜ê²Œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŒ.


### Confusion matrix

- True/False : ì‹¤ì œ ê°’ê³¼ ì˜ˆì¸¡ ê°’ì´ ì¼ì¹˜í•˜ëŠ”ì§€
- Positive/Negative : ì˜ˆì¸¡í•œ ê°’ì´ ì–‘ì„±ì¸ì§€ ìŒì„±ì¸ì§€

> Instruction :  
> 1. P, Nì„ ë¨¼ì € ìƒê°í•œë‹¤. ex) FNì´ë©´, ì¼ë‹¨ Negativeì´ë¯€ë¡œ ì˜ˆì¸¡ì€ negativeì„.  
> 2. ì´ì œ T, Fë¥¼ ìƒê°í•œë‹¤. Fì´ë¯€ë¡œ, negativeì™€ ë°˜ëŒ€ì¸ positiveê°€ ground truthì„ì„ ì•Œ ìˆ˜ ìˆìŒ.

- Confusion matrix : TP, FN, FP, TNì„ ê°ê° matrixì— ë‚˜íƒ€ë‚¸ ê²ƒ.

| ì‹¤ì œ ê°’ : ì˜ˆì¸¡ê°’ | ì–‘ì„± | ìŒì„± |
| ---------- | -- | -- |
| ì–‘ì„±         | TP | FN |
| ìŒì„±         | FP | TN |

- TP(true positive) : Aë¥¼ Aë¼ê³  ì˜ˆì¸¡í•œ ê²ƒ
- TN(true negative) : ~Aë¥¼ ~Aë¼ê³  ì˜ˆì¸¡í•œ ê²ƒ
- FP(false positive) : ~Aë¥¼ Aë¼ê³  ì˜ˆì¸¡í•œ ê²ƒ
- FN(false negative) : Aë¥¼ ~Aë¼ê³  ì˜ˆì¸¡í•œ ê²ƒ

ë‹¨ binary classificationì´ ì•„ë‹Œ ê²½ìš°ì—ë„ confusion matrixë¥¼ ì ìš©í•  ìˆ˜ ìˆìŒ.


| ground truth : prediction | A | B  | C  | D  |
| ------------------------- | - | -- | -- | -- |
| A                         | 9 | 1  | 0  | 0  |
| B                         | 1 | 15 | 3  | 1  |
| C                         | 5 | 0  | 24 | 1  |
| D                         | 0 | 4  | 1  | 15 |

- ì •í™•ë„(accuracy) in confusion matrix :

{% raw %}
$$
acc = \frac{TP+TN} {TP+TN+FP+FN}
$$
{% endraw %}


- Multiclassì˜ ê²½ìš° diagonal(only true positive)ë§Œì„ ëª¨ë‘ ë”í•´ mìœ¼ë¡œ ë‚˜ëˆˆ ê°’ì„ ì˜ë¯¸
- ìœ„ ì˜ˆì‹œì—ì„œì˜ ì •í™•ë„ëŠ” $(9+15+24+15)/80 = 0.78$

### Precision

- ì •ë°€ë„. positiveë¼ê³  ì˜ˆì¸¡í•œ ê²ƒ ë“¤ ì¤‘ ì‹¤ì œë¡œ positiveì¸ ê²ƒë“¤ì˜ ë¹ˆë„

{% raw %}
$$
prec = \frac{TP} {TP+FP}
$$
{% endraw %}



### Recall

- ì¬í˜„ìœ¨. ground truthê°€ positiveì¸ ê²ƒë“¤ ì¤‘ ëª¨ë¸ì´ positiveë¼ê³  ì˜ˆì¸¡í•œ ê²ƒì˜ ë¹ˆë„

{% raw %}
$$
rec = \frac{TP} {TP + FN}
$$
{% endraw %}



### P-R Curve, AUPRC


![](/assets/img/2023-09-30-[ë¨¸ì‹ ëŸ¬ë‹]-Chap-2---ëª¨ë¸-ì„ ì •ê³¼-ì„±ëŠ¥í‰ê°€/0.png)

- Precisionê³¼ Recallì€ ì¼ì¢…ì˜ tradeoffê°€ ì¡´ì¬í•¨.
- Precisionì„ yì¶•ì—, Recallì„ xì¶•ì— ë‘ê³  ê·¸ë˜í”„ë¥¼ ë§Œë“  ê²ƒì´ P-R curveì„.
- thresholdë¥¼ ì ì°¨ ë°”ê¾¸ì–´ ê°€ë©°, ê°ê°ì˜ confusion matrixë¡œë¶€í„° (P, R)ì„ êµ¬í•˜ëŠ” ë°©ë²•ì„ ì´ìš©.
- BEP (Break-Even Point)
: $y=x$ì™€ PR curveê°€ ë§Œë‚˜ëŠ” ì , PR curve ê°€ intersectí•  ê²½ìš° í‰ê°€í•˜ëŠ” ë°©ë²•ì¤‘ í•˜ë‚˜.
- ì¼ë°˜ì ìœ¼ë¡œ Area Under (PR) Curve (AUPRC)ê°€ í° classifierë¥¼ ì¢‹ì€ classifierë¼ê³  ë´„.
- ê·¸ëŸ¬ë‚˜ PR curve ê°„ì— crossê°€ ì¼ì–´ë‚˜ëŠ” ê²½ìš°ì—ëŠ” AUCë§Œìœ¼ë¡œ ë¹„êµí•˜ê¸°ê°€ ì• ë§¤í•¨.
- P, Rì˜ ì •ì˜ì— ì˜í•´ì„œ AUC=1ì¸, 1x1ì˜ square í˜•íƒœë¡œ ë‚˜íƒ€ë‚˜ëŠ” ê²ƒì´ ê°€ì¥ ì´ìƒì ì„.
- $(1, 1)$ì— ê°€ì¥ ê°€ê¹Œìš´ thresholdë¥¼ ê³ ë¥´ëŠ” ê²ƒì´ ì´ìƒì .

### F1-score

- ì •ë°€ë„, ì¬í˜„ìœ¨ ë‘ ê°’ì„ ì¡°í™” í‰ê· í•˜ì—¬ í•˜ë‚˜ì˜ ìˆ˜ì¹˜ë¡œ ë‚˜íƒ€ë‚¸ ì§€í‘œ
- ì •ë°€ë„($P$), ì¬í˜„ìœ¨($R$)ë¼ í•  ë•Œ

{% raw %}
$$
\text{F1 score = } \frac{2PR}{P+R}
$$
{% endraw %}



### F-beta-score

- F1-scoreëŠ” Precision, Recallì˜ ê°€ì¤‘ì¹˜ë¥¼ ë™ì¼í•˜ê²Œ ë‘” ê²½ìš°ì´ë©°, ì‹¤ì œ ìƒí™©ì—ì„œ ì–´ëŠ íŠ¹ì • ê°’ì— ê°€ì¤‘ì¹˜ë¥¼ ë‘ì–´ì•¼ í•  ê²½ìš° F-beta scoreë¥¼ ì‚¬ìš©í•¨. $\beta>1$ì¸ ê²½ìš°ëŠ” $R$ì˜ ì˜í–¥ì´ ë” í¬ë©°, $\beta<1$ì˜ ê²½ìš°ëŠ” $P$ì˜ ì˜í–¥ì´ ë” í¼.

{% raw %}
$$
F_\beta = \frac{(1+\beta^2)\times P \times R} {(\beta^2\times P) + R}
$$
{% endraw %}



### (Macro, Micro) P, R, F1-score

- ì—¬ëŸ¬ ê°œì˜ í˜¼ë™ í–‰ë ¬ì„ ì–»ê²Œ ë  ê²½ìš°, ê° Confusion matrixì—ì„œ ì–»ì€ P, R, F1ê°’ë“¤ì„ ê³„ì‚°í•˜ê³  ì´ ê°’ë“¤ì„ í‰ê· í•œ ê²ƒì´ macro-P, macro-R, macro-F1 ì´ë¼ í•œë‹¤.
- ë°˜ëŒ€ë¡œ ê°ê°ì˜ í˜¼ë™í–‰ë ¬ì´ ëŒ€ì‘í•˜ëŠ” ì›ì†Œë“¤ì— ëŒ€í•œ TP, FP, TN, FNë“¤ì˜ í‰ê· ê°’ì„ êµ¬í•˜ê³  ì´ë¥¼ ì´ìš©í•˜ì—¬ P, R, F1ê°’ì„ ê³„ì‚°í•˜ë©´ ì´ë¥¼ micro-P, micro-R, micro-F1ì´ë¼ í•œë‹¤.

### ROC, AUC

- thresholdë¥¼ ì •í•˜ê¸° ìœ„í•œ ë„êµ¬ : ROC

{% raw %}
$$
TPR = \frac{TP}{TP+FN}
$$
{% endraw %}



{% raw %}
$$
FPR = \frac{FP}{TN+FP}
$$
{% endraw %}


- True Positive Rate (TPR)
: ground truthê°€ Trueì¸ ê²ƒ ë“¤ ì¤‘ì—ì„œ classifierê°€ ì‹¤ì œë¡œ Trueë¼ê³  í•œ ê²ƒì˜ ë¹„ìœ¨ (Recall)
- False Postive Rate (FPR)
: ground truthê°€ falseì¸ ê²ƒ ë“¤ ì¤‘ì—ì„œ classifierê°€ ì‹¤ì œë¡œ Falseë¼ê³  í•œ ê²ƒì˜ ë¹„ìœ¨
- TPRì„ yì¶•, FPRì„ xì¶•ì— ë†“ê³  ê·¸ë˜í”„ë¡œ í‘œí˜„í•˜ ê²ƒì´ RPC ê·¸ë˜í”„
- y=xëŠ” ëœë¤ ì˜ˆì¸¡ ëª¨ë¸ì˜ ê²½ìš°ì¼ ê²ƒì´ë©°, AUCê°€ í´ ìˆ˜ë¡ ë” ì¢‹ì€ ëª¨ë¸ì„.
- AUCëŠ” ìˆœì„œ ì˜ˆì¸¡ í’ˆì§ˆ, ì¦‰ ìˆœì„œì˜ lossì™€ í° ê´€ë ¨ì´ ìˆìŒ $AUC=1-l_{rank}$

### Cost-sensitive error rate, cost curve

- FN, FPê°€ ì„œë¡œ ë‹¤ë¥¸ costë¥¼ ê°€ì§€ëŠ” ê²½ìš° unequal costë¼ëŠ” ê°œë… ì ìš©ê°€ëŠ¥.
- ë„ë©”ì¸ ì§€ì‹ì— ê¸°ë°˜ìœ¼ë¡œ cost matrixë¥¼ ìƒì„±

| ì‹¤ì œ ê°’ : ì˜ˆì¸¡ê°’ | ì–‘ì„±          | ìŒì„±          |
| ---------- | ----------- | ----------- |
| ì–‘ì„±         | 0           | $cost_{01}$ |
| ìŒì„±         | $cost_{10}$ | 0           |

- ë¹„ê· ë“± ë¹„ìš©ì—ì„œëŠ” ì˜¤ì°¨ íšŸìˆ˜ì˜ ìµœì†Œí™”ê°€ ì•„ë‹Œ ë¹„ìš©(total cost)ì˜ ìµœì†Œí™”ë¥¼ ëª©ì ìœ¼ë¡œ í•˜ë¯€ë¡œ, cost-sensitive error rateëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ëœë‹¤.

{% raw %}
$$
\begin{aligned}E(f;D;cost) = \frac{1}{m}\bigg(\sum_{{\bf x_i}\in D^+}\text{II}(f({\bf x_i})\neq y_i) \times cost_{01}+ \\ \sum_{{\bf x_i}\in D^-}\text{II}(f({\bf x_i})\neq y_i) \times cost_{10}\bigg)\end{aligned}
$$
{% endraw %}



### Cost curve

- x axis of cost curves is probability cost of positive class

{% raw %}
$$
P(+)cost = \frac{p \times cost_{01}}{p\times cost_{01}+(1-p)\times cost_{10}}
$$
{% endraw %}


- $p\in [0,1]$ ëŠ” sampleì´ positiveì¼ í™•ë¥ . $p=0$ì¼ ë•Œ $P(+)cost =0$ì´ê³ , 1ì¼ ë•Œ 1ì´ë‹ˆ ë¹„ìš©ê°€ì¤‘(ë¹„ìš©ì„ ê³ ë ¤í•œ) sampleì´ positiveí•  í™•ë¥  ì„ xì¶•ì— ë†“ì•˜ë‹¤ê³  ë³¼ ìˆ˜ ìˆìŒ.
- y axis is normalized cost which takes values from [0,1]

{% raw %}
$$
cost_{norm} = \frac{FNR \times p \times cost_{01} + FPR \times (1-p)\times cost_{10}} {p\times cost_{01} + (1-p)\times cost_{10}}
$$
{% endraw %}


- ë‚´ê°€ pì˜ í™•ë¥ ì— ë”°ë¼ ë¶„ë¥˜í•œ ê²ƒì´ ë‹¤ í‹€ë ¸ì„ ë•Œì˜ ì „ì²´ costë¥¼ ë¶„ëª¨ë¡œ, pì˜ í™•ë¥ ë¡œ ë¶„ë¥˜í•˜ë‚˜ FNR, FPRì„ ê³ ë ¤í•˜ì—¬ ë¶„ë¥˜í–ˆì„ ë•Œì˜ costë¥¼ ë¶„ìë¡œ í•˜ëŠ”ëŠë‚Œ? ìµœì•…ì˜ ê²½ìš°ì˜ ìµœëŒ€ cost ë¶„ì— í˜„ì¬ í•™ìŠµê¸°ì˜ costë¥¼ normalized costë¼ê³  í•˜ëŠ” ë“¯
- $FNR = 1-TPR$
- $p=0$ì¼ ë•Œì˜ $cost_{norm}=FNR$, $p=1$ì¼ ë•Œì˜ $cost_{norm} = FPR$ì´ë¯€ë¡œ, cost-plane ìƒì— ê°ê°ì˜ ROC curve ì˜ ì ë“¤ì— ëŒ€í•œ $(0,FPR), (1,1-TPR)$ì„ ì‡ëŠ” ì§ì„ ë“¤ì„ ëª¨ë‘ plot í•˜ì—¬ ì§ì„ ë“¤ ì•„ë˜ì˜ ë©´ì ì´ ê¸°ëŒ€ ì´ ë¹„ìš© (expected total cost)ê°€ ë  ê²ƒ.

![](/assets/img/2023-09-30-[ë¨¸ì‹ ëŸ¬ë‹]-Chap-2---ëª¨ë¸-ì„ ì •ê³¼-ì„±ëŠ¥í‰ê°€/1.png)


## 2.4 í•™ìŠµê¸° ì„±ëŠ¥ ë¹„êµ ê²€ì¦


### ì´í•­ ê²€ì • (binomial test)

- $\epsilon\leq\epsilon_0$ ë¼ëŠ” ê°€ì„¤ì„ ì„¸ìš°ê³  $\alpha=0.05$ ë“±ì˜ significance í•˜ì—ì„œ ê°€ì„¤ì„ ê¸°ê°í•  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸.
- í•´ë‹¹ ì˜¤ì°¨ìœ¨($\epsilon_0$)ë³´ë‹¤ ì˜¤ì°¨ìœ¨ì´ í´ í™•ë¥ ê³¼ $\alpha$ì™€ ë¹„êµ

### t-test (ì—¬ëŸ¬ test-setì— ëŒ€í•œ ì˜¤ì°¨ìœ¨ì„ ê²€ì •í•  ë•Œ)

- kê°œì˜ í…ŒìŠ¤íŠ¸ ì˜¤ì°¨ìœ¨ $\hat\epsilon_1, \hat\epsilon_2\cdots,\hat\epsilon_k$ì„ ì–»ëŠ”ë‹¤ë©´, ê°ê°ì˜ í‰ê·  ì˜¤ì°¨ìœ¨ $\mu$ì™€ ë¶„ì‚° $\sigma^2$ì„ ì–»ì„ ìˆ˜ ìˆìŒ.
- ì´ ë•Œ ê°ê°ì˜ í…ŒìŠ¤íŠ¸ ì˜¤ì°¨ìœ¨ì´ ë…ë¦½í‘œë³¸ì´ë¼ë©´ ê²€ì •í†µê³„ëŸ‰ì€ ë‹¤ìŒê³¼ ê°™ìŒ.

{% raw %}
$$
\tau_t = \frac{\sqrt k (\mu-\epsilon_0)}{\sigma}\sim t(k-1)
$$
{% endraw %}


- $\alpha$ì˜ significance í•˜ì—ì„œ, $\mu=\epsilon_0$ì˜ ê·€ë¬´ê°€ì„¤ì„ ê¸°ê°í•  ìˆ˜ ìˆëŠ” ê¸°ê°ì—­ì€ $(-\infty,t_{-\alpha/2}]$, $[t_{-\alpha/2}, \infty)$ ì´ë‹¤.

### êµì°¨ ê²€ì¦ t-test

- ë‘ í•™ìŠµê¸°ì˜ ì˜¤ì°¨ìœ¨ì´ ê°™ë‹¤ëŠ” ê·€ë¬´ê°€ì„¤ì„ ì„¸ìš°ê³ , ë™ì¼í•œ test-setì„ ì‚¬ìš©í•˜ì—¬ ë‚˜ì˜¨ ë‘ í•™ìŠµê¸°ì˜ ì˜¤ì°¨ìœ¨ì˜ ì°¨($\Delta_i$)ë¥¼ t-test í•œë‹¤.

{% raw %}
$$
\tau_t = \vert\frac{\sqrt{k}\mu}{\sigma}\vert
$$
{% endraw %}


- ê°™ì€ ë°©ë²•ìœ¼ë¡œ ê¸°ê°ì—­ì„ ì„¸ì›Œ t-test í•˜ë©´ ë¨.
- Test ë‚´ì˜ ì¤‘ë³µ ë°ì´í„° ë“±ì´ ì¡´ì¬í•  ê²½ìš° ì˜¤ì°¨ìœ¨ì´ ë…ë¦½ì ì´ì§€ ì•Šìœ¼ë¯€ë¡œ, 5 x 2 cross-validation ë“±ì„ í™œìš©.

### 5 x 2 cross-validation

- testing setì„ 5ê°œë¡œ ë‚˜ëˆ„ê³ , ê°ê°ì˜ setì„ ë˜ 2ê°œë¡œ ë‚˜ëˆ„ì–´ 2-fold cross-validationì„ ìˆ˜í–‰í•œë‹¤. 2ê°œì˜ ì˜¤ì°¨ìœ¨ ì°¨ì´ê°’ì´ ë„ì¶œ($\Delta_i^1, \Delta_i^2 \text{ for i=1}\cdots 5$)
- ë¹„ë…ë¦½ì„±ì˜ ì™„í™”ë¥¼ ìœ„í•´(?) i=1ì¼ë•Œì˜ ì˜¤ì°¨ìœ¨ì˜ í‰ê· ì„ $\mu$ë¡œ í™œìš©
- ë¶„ì‚°ì€ ê°ê°ì˜ 5ê°œì— ëŒ€í•œ ë¶„ì‚°ì˜ í•©ì„ ì´ìš©.

{% raw %}
$$
\sigma_i^2 = \bigg(\Delta_i^1-\frac{\Delta_i^1+\Delta_i^2}{2}\bigg)^2+\bigg(\Delta_i^2-\frac{\Delta_i^1+\Delta_i^2}{2}\bigg)^2
$$
{% endraw %}


- ì´ë ‡ê²Œ ê³„ì‚°ëœ Tê²€ì •í†µê³„ëŸ‰ì€

{% raw %}
$$
\tau_t = \frac{\mu}{\sqrt{0.2 \sum_{i=1}^5 \sigma_i^2}}
$$
{% endraw %}



### ë§¥ë‹ˆë§ˆ(McNemar) test

- contingency table of two learners

| Alg.A : Alg.B | Correct  | Incorrect |
| ------------- | -------- | --------- |
| Correct       | $e_{00}$ | $e_{01}$  |
| Incorrect     | $e_{10}$ | $e_{11}$  |

- ë‘ í•™ìŠµê¸°ì˜ ì„±ëŠ¥ì´ ë™ì¼í•˜ë‹¤ëŠ” ê²ƒì€ $e_{01}=e_{10}$ì„. ì¦‰ ì„œë¡œ ë‹¤ë¥´ê²Œ íŒë³„í•œ ê°œìˆ˜ì˜ ì°¨ê°€ ì •ê·œë¶„í¬ë¥¼ ì´ë£° ê²ƒì´ë¯€ë¡œ,

{% raw %}
$$
\tau_{\chi^2} = \frac{(\vert e_{01}-e_{10}\vert -1)^2}{e_{01}+e_{10}} \sim \chi^2_1
$$
{% endraw %}


- ìœ„ ê²€ì •í†µê³„ëŸ‰ì€ ììœ ë„ê°€ 1ì¸ ì¹´ì´ì œê³± ë¶„í¬ë¥¼ ë”°ë¥´ê²Œ ë  ê²ƒì´ë‹¤

### í”„ë¦¬ë“œë¨¼(Friedman) test

- ë‹¤ìˆ˜ì˜ ì•Œê³ ë¦¬ì¦˜ì˜ Rankingì„ ë§¤ê¸¸ ë•Œ ì‚¬ìš©í•˜ëŠ” ê²€ì •ë°©ë²•
- ì•ì„  ê²€ì¦ë²•ë“¤ì„ ì´ìš©í•˜ì—¬ ê° data set, ê° algorithmë³„ í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ì–»ê³  ìˆœìœ„ë¥¼ ë§¤ê¹€ (ë‹¨, ì„±ëŠ¥ê°’ì´ ëª¨ë‘ ê°™ë‹¤ë©´ í‰ê·  ê°’ì„ ë§¤ê¹€)
- ê°ê°ì˜ ì•Œê³ ë¦¬ì¦˜ì˜ í‰ê·  ë“±ìˆ˜ì— ëŒ€í•œ í‰ê· ê³¼ ë¶„ì‚°ì„ êµ¬í•  ìˆ˜ ìˆìœ¼ë©°, Nê°œì˜ data set, kê°œì˜ algorithmì— ëŒ€í•´

{% raw %}
$$
\tau_{\chi^2} = \frac{k-1}{k} \cdot \frac{12N}{k^2-1} \sum_{i=1}^k \bigg(r_i-\frac{k+1}{2}\bigg)^2 \sim \chi^2_{k-1}
$$
{% endraw %}


- ìœ„ ê²€ì • í†µê³„ëŸ‰ì€ ììœ ë„ê°€ k-1ì¸ ì¹´ì´ì œê³± ë¶„í¬ë¥¼ ì‚¬ìš©.
- í”„ë¦¬ë“œë¨¼ ê²€ì •ì´ ë³´ìˆ˜ì ì´ë¯€ë¡œ(ì°¨ì´ê°€ ì—†ë‹¤ê³  íŒë³„í•  ê°€ëŠ¥ì„±ì´ í¼) ê°œì„ ëœ í”„ë¦¬ë“œë¨¼ ê²€ì •ì„ ì‚¬ìš©.

{% raw %}
$$
\tau_F = \frac{(N-1)\tau_{\chi^2}}{N(k-1)-\tau_{\chi^2}}\sim F_{k-1, (k-1)(N-1)}
$$
{% endraw %}


- ìœ„ ê²€ì • í†µê³„ëŸ‰ì€ ììœ ë„ $k-1$, $(k-1)(N-1)$ì„ ë”°ë¥´ëŠ” Fë¶„í¬ì„.

### ë„¤ë©”ë‹ˆ ì‚¬í›„ ê²€ì •(Nemenyi post-hoc test)


{% raw %}
$$
CD = q_\alpha \sqrt{\frac{k(k+1)}{6N}}
$$
{% endraw %}


- q : Tukey ë¶„í¬
- CD ê°’ì„ ê³„ì‚°í•˜ì—¬ì„œ ë‘ ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ì˜ í‰ê· ê°’ ì°¨ì´ê°€ í•´ë‹¹ ìˆ˜ì¹˜ ì´ìƒì¼ ê²½ìš° ë‘ ì•Œê³ ë¦¬ì¦˜ì€ ì°¨ì´ê°€ ìˆë‹¤ê³  ë³¼ ìˆ˜ ìˆìŒ.

## 2.5 í¸í–¥ê³¼ ë¶„ì‚°

- $\bf x$ : í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ
- $y_D$ : label
- $y$ : ground-truth (ì‹¤ì œ ë°ì´í„° ê°’)
- $f({\bf x}, D)$ : ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’
- ì•Œê³ ë¦¬ì¦˜ì˜ ê¸°ëŒ€ ì˜ˆì¸¡ê°’ (ì˜ˆì¸¡ê°’ì˜ í‰ê·  or ê¸°ëŒ“ê°’)

{% raw %}
$$
\bar f({\bf x}) = E_D[f({\bf x}, D)]
$$
{% endraw %}


- ë¶„ì‚°

{% raw %}
$$
var({\bf x}) = E_D[(f({\bf x}, D)-\bar f({\bf x}))^2]
$$
{% endraw %}


- ë…¸ì´ì¦ˆ

{% raw %}
$$
\epsilon^2 = E_D[(y_D-y)^2]
$$
{% endraw %}


- í¸í–¥(ê¸°ëŒ€ ê²°ê´ê°’ê³¼ ì‹¤ì œ ë°ì´í„°ì˜ ì°¨ì´)

{% raw %}
$$
bias^2({\bf x}) = (\bar f({\bf x})-y)^2
$$
{% endraw %}


- ì¼ë°˜ ì˜¤ì°¨(Total error)ê°€ í¸í–¥, ë¶„ì‚°, ë…¸ì´ì¦ˆì˜ í•©ìœ¼ë¡œ ë¶„í•´ë  ìˆ˜ ìˆìŒ.

{% raw %}
$$
\begin{aligned} Error(f;D) =& E_D [(f({\bf x};D)-y_D)^2]\\ =& E_D[(f({\bf x};D)-\bar f({\bf x})+\bar f({\bf x})-y_D)^2]\\ =& var({\bf x}) + E_D[(\bar f({\bf x})-y_D)^2]\\ &+ E_D[2(f({\bf x};D)-\bar f({\bf x}))(\bar f({\bf x})-y_D)] \\ =& var({\bf x}) + E_D[(\bar f({\bf x})-y+y-y_D)^2] \\ =& var({\bf x})+((\bar f({\bf x})-y)^2+E_D[(y_D-y)^2]\end{aligned}
$$
{% endraw %}



{% raw %}
$$
\therefore Error(f;D) = bias^2({\bf x})+ var({\bf x})+\epsilon^2
$$
{% endraw %}



<script>
  window.MathJax = {
    tex: {
      macros: {
        R: "\\mathbb{R}",
        N: "\\mathbb{N}",
        Z: "\\mathbb{Z}",
        Q: "\\mathbb{Q}",
        C: "\\mathbb{C}",
        proj: "\\operatorname{proj}",
        rank: "\\operatorname{rank}",
        im: "\\operatorname{im}",
        dom: "\\operatorname{dom}",
        codom: "\\operatorname{codom}",
        argmax: "\\operatorname*{arg\,max}",
        argmin: "\\operatorname*{arg\,min}",
        "\{": "\\lbrace",
        "\}": "\\rbrace",
        sub: "\\subset",
        sup: "\\supset",
        sube: "\\subseteq",
        supe: "\\supseteq"
      },
      tags: "ams",
      strict: false, 
      inlineMath: [["$", "$"], ["\\(", "\\)"]],
      displayMath: [["$$", "$$"], ["\\[", "\\]"]]
    },
    options: {
      skipHtmlTags: ["script", "noscript", "style", "textarea", "pre"]
    }
  };
</script>
<script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
