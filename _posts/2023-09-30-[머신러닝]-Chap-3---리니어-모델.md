---
layout: post
title: "[ë¨¸ì‹ ëŸ¬ë‹] Chap 3 - ë¦¬ë‹ˆì–´ ëª¨ë¸"
date: 2023-09-30
draft: false
published: true
pin: false
image:
  path: "/assets/img/2023-09-30-[ë¨¸ì‹ ëŸ¬ë‹]-Chap-3---ë¦¬ë‹ˆì–´-ëª¨ë¸/0.png"
  alt: "[ë¨¸ì‹ ëŸ¬ë‹] Chap 3 - ë¦¬ë‹ˆì–´ ëª¨ë¸"
description: "ë¨¸ì‹ ëŸ¬ë‹ì˜ ë¦¬ë‹ˆì–´ ëª¨ë¸ì— ëŒ€í•œ ë‚´ìš©ìœ¼ë¡œ, ê¸°ë³¸ í˜•ì‹, ì„ í˜• íšŒê·€, ë¡œì§€ìŠ¤í‹± íšŒê·€, ì„ í˜• íŒë³„ ë¶„ì„(LDA), ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ ë° í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œë¥¼ ë‹¤ë£¹ë‹ˆë‹¤. ì£¼ìš” ê°œë…ìœ¼ë¡œëŠ” MSE ìµœì†Œí™”, ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜, ìµœëŒ€ ìš°ë„ ì¶”ì •, LDAì˜ ë¶„ì‚° í–‰ë ¬, ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ ì „ëµ(OvO, OvR, MvM) ë° í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œ í•´ê²° ë°©ë²•ì´ í¬í•¨ë©ë‹ˆë‹¤."
tags: ["Linear regression", "Logistic regression", "LDA"]
categories: ["Lecture", "Machine Learning"]
math: true
---


### Disclaimer


{: .prompt-info }


> ğŸ“£ ë³¸ í¬ìŠ¤íŠ¸ëŠ” ì¡°ìš°ì¯”í™”ì˜ [ë‹¨ë‹¨í•œ ë¨¸ì‹ ëŸ¬ë‹](https://product.kyobobook.co.kr/detail/S000001916959) ì±…ì„ ìš”ì•½ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤. 


# Chap 3. Linear Model


## 3.1 ê¸°ë³¸ í˜•ì‹

- dê°œì˜ ì†ì„±ì„ ê°€ì§„ ìƒ˜í”Œ $x=(x_1, x_2, x_3, \cdots , x_d)$

{% raw %}
$$
f({\bf x}) = w_1x_1 + w_2x_2 + \cdots + w_dx_d + b \\ f({\bf x}) = {\bf w^T x }+ b
$$
{% endraw %}


- í•™ìŠµì„ í†µí•´ wì™€ bë¥¼ ê²°ì •. (ê³„ìˆ˜ ê²°ì •)

## 3.2 Linear regression

- eg) í•˜ë‚˜ì˜ propertyë§Œì„ ê°€ì§„ Data set Dë¥¼ ê³ ë ¤í•˜ì

{% raw %}
$$
\text{consider } D = \{(x_1, y_1), \cdots, (x_m,y_m)\}
$$
{% endraw %}


- Linear regression aims to learn the function:

{% raw %}
$$
\begin{aligned}f(x) =& wx+b, \ \text{s.t. } f(x_i)\approx y_i \ (i=1\cdots m)\\ ( w^*, b^*) =& argmin_{\substack{(w,b)}}\sum_{i=1}^m (f(x_i)-y_i)^2\end{aligned}
$$
{% endraw %}


- Minimize the mean-square error(MSE)

{% raw %}
$$
\begin{aligned}&E_{(w,b)} = \sum_{i=1}^m (y_i-wx_i-b)^2\\ &\frac{\partial E_{(w,b)}}{\partial w}=2\bigg(w\sum_{i=1}^m x_i^2-\sum_{i=1}^m(y_i-b)x_i\bigg)\\ &\frac{\partial E_{(w,b)}}{\partial b}= 2\bigg(mb-\sum_{i=1}^m(y_i-wx_i)\bigg)\\ \therefore \ &w = \frac{\sum_{i=1}^m y_i(x_i-\bar x)}{\sum_{i=1}^m x_i^2-\frac{1}{m}\bigg(\sum_{i=1}^m x_i\bigg)^2} \\ &b = \frac{1}{m}\sum_{i=1}^m(y_i-wx_i)\end{aligned}
$$
{% endraw %}


- Multivariate (${\bf x} = (x_1, x_2, \cdots, x_d)$)
- Datasetì„ matrix formìœ¼ë¡œ ë‚˜íƒ€ë‚´ ë³´ì

{% raw %}
$$
X = \begin{pmatrix} x_{11} & x_{12} & \cdots & x_{1d} & 1 \\ x_{21} & x_{22} & \cdots & x_{2d} & 1 \\ \vdots & \vdots & \ddots & \vdots & \vdots \\ x_{m1} & x_{m2} & \cdots & x_{md} & 1 \end{pmatrix} = \begin{pmatrix} x_1^T & 1 \\ x_2^T & 1 \\ \vdots & \vdots\\ x_m^T & 1 \end{pmatrix}
$$
{% endraw %}



{% raw %}
$$
\hat{\bf w}^* = argmin_{\hat{\bf w}} ({\bf y - X\hat w^T})({\bf y - X\hat w})
$$
{% endraw %}



{% raw %}
$$
\frac{\partial E_{\hat{\bf w}}}{\partial {\hat{\bf w}}}= 2{\bf X^T(X\hat w - y)}
$$
{% endraw %}


- The solution of multivariate problem

{% raw %}
$$
\begin{aligned} \text{when }\hat{\bf w} =& \ ({\bf w};b)\\ {\bf \hat{w}^*} =& \ {\bf (X^T X)^{-1}X^T y} \\ f({\bf \hat{x_i}})=& \  {\bf \hat{x_i}}^T {\bf (X^T X)^{-1} X^T y}\end{aligned}
$$
{% endraw %}


- Log linear regression (special case of generalized linear models)
	- Dataê°€ ì§€ìˆ˜ ì²™ë„ì—ì„œ ë³€í™”í•œë‹¤ë©´ ì‚¬ìš©

{% raw %}
$$
\begin{aligned}\ln y =& \ {\bf w^T x}+ b \\ y =&  \ g^{-1}({\bf w^T x + b})\end{aligned}
$$
{% endraw %}


- for generalized linear models, $g(\cdot )$ is monotonic differentible function (link function)
- $g(\cdot) = \ln(\cdot)$

## 3.3 Logistic Regression


### Binary Classification

- Linear modelì„ ì‚¬ìš©í•˜ëŠ”ë°, ground truthê°€ $\{0,1\}$ì¸ ê²½ìš°
- $z = {\bf w^T x }+ b \in R$ ì´ë¯€ë¡œ, ì—°ê²°í•´ì¤„ link functionì„ ì°¾ì•„ì•¼ í•¨.
- Unit-step function(Heaviside function)

{% raw %}
$$
u(z) = \begin{cases} 0, & z<0;\\ 1/2, & z=0;\\1, & z>0\end{cases}
$$
{% endraw %}


- Unit step functionì€ link functionìœ¼ë¡œ í™œìš©í•  ìˆ˜ ì—†ìŒ (ë¯¸ë¶„ë¶ˆê°€ëŠ¥í•œì ì´ ì¡´ì¬í•˜ì—¬ $g^{-1}$ì´ ì •ì˜ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ)
- Sigmoid function

{% raw %}
$$
y=\frac{1}{1+e^{-x}}
$$
{% endraw %}


- sigmoid functionì€ differentibleí•˜ê³  monotoicí•˜ë¯€ë¡œ, ì´ë¥¼ ë³€í™˜ì‹œì¼œì£¼ë©´,

{% raw %}
$$
\ln{\frac{y}{1-y}}= {\bf w^T x }+ b
$$
{% endraw %}


- $y$ : $x>0$ì¼ ê°€ëŠ¥ì„±, $1-y$ : $x<0$ì¼ ê°€ëŠ¥ì„±ìœ¼ë¡œ, ln í•­ì—ì„œ ë‘˜ì„ ë¹„êµí•˜ëŠ” ê²ƒìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŒ. ì´ëŸ¬í•œ ë¶„ìˆ˜ë¥¼ oddsë¼ê³  ë¶€ë¦„

{% raw %}
$$
\text{odds} := \frac{y}{1-y}
$$
{% endraw %}


- oddsì— ë¡œê·¸ë¥¼ ì·¨í•œ ê²ƒì„ logitì´ë¼ ë¶€ë¦„

{% raw %}
$$
\text{logit} := \ln{\frac{y}{1-y}}
$$
{% endraw %}



### applying maximum likelihood

- logits can be rewritten as

{% raw %}
$$
\begin{aligned}\ln{\frac{p(y=1\vert {\bf x})}{p(y=0\vert {\bf x})}} =& \  {\bf w^T x }+ b \\ p(y=1\vert {\bf x}) =& \  \frac{e^{w^Tx+b}}{1+e^{w^T x + b}} \\ p(y=0\vert {\bf x}) =& \  \frac{1}{1+e^{w^T x + b}}
	
	\end{aligned}
$$
{% endraw %}


- Maximum likelihood methodë¥¼ í†µí•´ $w, b$ë¥¼ ì¶”ì¸¡í•˜ë©´,

{% raw %}
$$
l({\bf w}, b) = \sum_{i=1}^m \ln {p(y_i \vert {\bf x_i}; {\bf w}, b)}
$$
{% endraw %}


- ì´ ë•Œ $\beta = ({\bf w};b), \hat{\bf x} = ({\bf x}; 1)$ ë¡œ ê°€ì •,

{% raw %}
$$
l({\bf w}, b) = \sum_{i=1}^m \ln{y_ip_1(\hat {\bf x_i};\beta)+ (1-y_i)p_0(\hat {\bf x_i};\beta)}
$$
{% endraw %}



{% raw %}
$$
l({\bf \beta}) = \sum_{i=1}^m \bigg(-y_i{\bf \beta^T}\hat{\bf x_i} + \ln{\big(1+e^{{\bf \beta^T}\hat{\bf x_i}}\big)}\bigg)
$$
{% endraw %}


- find beta with gradient descent, Newton method

{% raw %}
$$
{\bf \beta^*} = argmin_{\beta} \ l(\beta)
$$
{% endraw %}


- eg) Newton's Method,

{% raw %}
$$
\begin{aligned}{\bf \beta}^{t+1} =& \ {\bf \beta}^t - \bigg(\frac{\partial^2 l(\beta)}{\partial \beta\partial \beta^T}\bigg) \frac{\partial l(\beta)}{\partial \beta} \\ \frac{\partial l(\beta)}{\partial \beta} =& \ \sum_{i=1}^m \hat{\bf x_i} (y_i-p_1(\hat{\bf x_i};{\bf \beta})) \\ \bigg(\frac{\partial^2 l(\beta)}{\partial \beta\partial \beta^T}\bigg) =& \ \sum_{i=1}^m \hat{\bf x_i}\hat{\bf x_i}^T p_1(\hat{\bf x_i};{\bf \beta}) (1-p_1(\hat{\bf x_i};{\bf \beta}))
	
	\end{aligned}
$$
{% endraw %}



## 3.4 Linear Discriminant Analysis (LDA)


![](/assets/img/2023-09-30-[ë¨¸ì‹ ëŸ¬ë‹]-Chap-3---ë¦¬ë‹ˆì–´-ëª¨ë¸/0.png)

- Data pointsë¥¼ $y=wx+b$ì— íˆ¬ì˜ì‹œí‚¤ê³ , ê°™ì€ labelì„ ê°€ì§„ ì ë“¤ì˜ ìœ„ì¹˜ê°€ ìµœëŒ€í•œ ê°€ê¹ë„ë¡ $w$ì™€ $b$ë¥¼ ì°¾ëŠ” ë°©ë²•
- ì¦‰ data setì´ givenì¼ ë•Œ ìµœëŒ€í™”ì‹œí‚¤ëŠ” $y=wx+b$ë¥¼ ì°¾ëŠ” ê²ƒ
- Some variables ($n$ : number of features, $m_i$ : number of data)
	- sample set of the ith class $X_i$ ($n\times m_i$)
	- mean vector of the ith class $ {\bf \mu_i}$ ($n \times 1$)
	- covariance matrix of the i-th class {$\bf \Sigma_i$} ($n \times n$)
	(ê°ê°ì˜ ë³€ìˆ˜ë“¤ì— ëŒ€í•œ ê³µë¶„ì‚°ì„ matrix í˜•íƒœë¡œ í‘œì‹œ)
- After projection to a line
	- the centers of those two classes samples ${\bf w}^T{\bf \mu}_0$, ${\bf w}^T{\bf \mu}_1$
	- the covariances of the two classes samples ${\bf w}^T{\bf \Sigma}_0{\bf w}$,  ${\bf w}^T{\bf \Sigma}_1{\bf w}$
- Objective to be maximized

{% raw %}
$$
J = \frac{\Vert{\bf w^T \mu_0 - w^T \mu_1}\Vert_2^2}{\bf w^T{\bf \Sigma_0}{\bf w}+ \bf w^T{\bf \Sigma_1}{\bf w}}
$$
{% endraw %}


- ë¶„ìì™€ ë¶„ëª¨ë¡œ ë‚˜ëˆ„ì–´ ìƒê°í•´ë³´ë©´ ë¶„ìëŠ” í‰ê· ì˜ 2-norm, ë¶„ëª¨ëŠ” co-variance ì˜ í•©ì„ì„ ì•Œ ìˆ˜ ìˆë‹¤.

{% raw %}
$$
J = \frac{ \bf w^T({\bf \mu_0-\mu_1})({\bf \mu_0-\mu_1})^T{\bf w}}{\bf w^T({\bf \Sigma_0 +\Sigma_1}){\bf w}}
$$
{% endraw %}


- The within-class scatter matrix

{% raw %}
$$
\begin{aligned}{\bf S_w} =& \ {\bf \Sigma_0 + \Sigma_1} \\ =& \ \sum_{{\bf x}\in X_0}({\bf x-\mu_0})({\bf x-\mu_0})^T \\ +& \ \sum_{{\bf x}\in X_1}({\bf x-\mu_1})({\bf x-\mu_1})^T\end{aligned}
$$
{% endraw %}


- Between-class scatter matrix

{% raw %}
$$
{\bf S_b} =({\bf \mu_0-\mu_1})({\bf \mu_0-\mu_1})^T
$$
{% endraw %}


- Generalized Rayleigh quotient

{% raw %}
$$
J = \frac{\bf w^T S_b w}{\bf w^T S_w w}
$$
{% endraw %}


- ${\bf w^T S_w w} = 1$ì´ë¼ í•˜ë©´, maximizing generalized Reyleigh quotient is equivalent to

{% raw %}
$$
\begin{aligned}min_w -{\bf w^T S_b w} \\ \text{s.t.} {\bf w^T S_w w}=1
	
	\end{aligned}
$$
{% endraw %}


- Using the method of Lagrange multipliers
(ì €ëŸ° constraint ë¬¸ì œë¥¼ í‘¸ëŠ” ë°©ë²• : lagrangian multiplier)

{% raw %}
$$
{\bf S_b w} = \lambda {\bf S_w w}
$$
{% endraw %}


- recap the definition of Between class scatter matrix

{% raw %}
$$
{\bf S_b w} = \lambda ({\bf \mu_0-\mu_1})
$$
{% endraw %}


- ìœ„ ë‘ ì‹ì„ ì—°ë¦½í•˜ë©´,

{% raw %}
$$
{\bf w} = {\bf S_w^{-1}} ({\bf \mu_0-\mu_1})
$$
{% endraw %}


- Solve by singular value decomposition : ${\bf S_w} = {\bf U\Sigma V^T}$

### Extend LDA to multiclass classification problems

- The global scatter matrix

{% raw %}
$$
\begin{aligned}{\bf S_t} =& \ {\bf S_b} + {\bf S_w} \\ =& \ \sum_{i=1}^m ({\bf x_i-\mu})({\bf x_i-\mu})^T
	
	\end{aligned}
$$
{% endraw %}


- Decomposition of within-class scatter matrix

{% raw %}
$$
\begin{aligned}{\bf S_w} =& \sum_{i=1}^N {\bf S_{w_i}}\\ \text{where } \ S_{w_i} =& \sum_{{\bf x}\in X_i} ({\bf x-\mu_i})({\bf x-\mu_i})^T \\ {\bf S_b} = \ {\bf S_t}-{\bf S_w} =&  \sum_{i=1}^N m_i ({\bf \mu_i-\mu})({\bf \mu_i-\mu})^T\end{aligned}
$$
{% endraw %}


- Think about the formula $Var(X) = E(X^2)-E(X)^2$
- Objective :

{% raw %}
$$
\begin{aligned}\max_{\bf W}\ \ \frac{tr\bigg({\bf W^T S_b W}\bigg)}{tr\bigg({\bf W^T S_w W}\bigg)} \\ \text{where }{\bf W}\in {\bf R}^{d\times (N-1)}
	
	\end{aligned}
$$
{% endraw %}


- (trace?) : ê°ê°ì— ëŒ€í•œ w vectorë“¤ì€ ì„œë¡œì„œë¡œë¼ë¦¬ë§Œ ê³±í•´ì ¸ì•¼í•˜ë¯€ë¡œ ê´€ì‹¬ìˆëŠ” í•­ì€ diagonalì„.
ì—¬ê¸°ì„œì˜ $d$ : $n$ (number of features), $N$ : number of classes
- which leads to lagrange multiplier problem

{% raw %}
$$
{\bf S_B W} = \lambda {\bf S_w W}
$$
{% endraw %}


- $W$ corresponds to d' largest non-zero eigenvalues of ${\bf S_w^{-1} S_b}$ (d'ê°œì˜ non-zero eigenvaluesë¡œë¶€í„° êµ¬í•œ eigenvectorë¥¼ concatí•œ ê²Œ Wì´ë‹¤?) where $d'\leq N-1$

## 3.5 Multiclass Classification

- Nê°œ í´ë˜ìŠ¤ë¥¼ ë¶„ë¥˜í•˜ëŠ” ë¶„í•´ë²•. Binary classificationì˜ í™•ì¥ìœ¼ë¡œ, 3ê°€ì§€ ì •ë„ì˜ ì „í˜•ì  ë¶„í•´ ì „ëµì´ ìˆìŒ
- OvO (One vs. One)
- OvR (One vs. Rest)
- MvM (Many vs. Many)

### OvO (One vs One classification)

- Nê°œì˜ í´ë˜ìŠ¤ì—ì„œ 2ê°œì”© ì„ íƒí•¨. ì¦‰ $_nC_2$ê°œì˜ Binary classification ë¬¸ì œ ìƒì„±
- ê²°ê´ê°’ì˜ ê²°ì •ì€ ê°€ì¥ ë§ì€ ì„ íƒì„ ë°›ì€ ê²°ê³¼ë¥¼ ì„ íƒí•˜ê²Œ ë¨.

### OvR (One vs Rest classification)

- Nê°œì˜ í´ë˜ìŠ¤ ê°ê°ì„ ì–‘ì„±ìœ¼ë¡œ, ê·¸ ë‚˜ë¨¸ì§€ë¥¼ ìŒì„±ìœ¼ë¡œ í•˜ì—¬ Nê°œì˜ classifier í•™ìŠµ.
- í•œê°œë§Œ ì–‘ì„±ì´ë¼ë©´ ê·¸ labelì„, ì—¬ëŸ¬ ê°œê°€ ì–‘ì„±ì´ë¼ë©´ ê° classifierì˜ ì˜ˆì¸¡ ì‹ ë¢°ë„ê°€ ê°€ì¥ í° í´ë˜ìŠ¤ì˜ ë ˆì´ë¸”ì„ ê²°ê³¼ë¡œ ì„ íƒ
- OvO, OvRì€ ì—¬ëŸ¬ tradeoffê°€ ìˆì§€ë§Œ ê²°ê³¼ì ìœ¼ë¡œ ì„±ëŠ¥ì€ ë¹„ìŠ·í•¨

### MvM (Many vs Many classification)

- Nê°œì˜ classë“¤ì„ Më²ˆ positiveì™€ negative (í˜¹ì€ neutral)ë¡œ ë‚˜ëˆ”.
- base codewordë¥¼ ê³„ì‚°.

![](/assets/img/2023-09-30-[ë¨¸ì‹ ëŸ¬ë‹]-Chap-3---ë¦¬ë‹ˆì–´-ëª¨ë¸/1.png)

- columnì— ê° classifierë“¤ì„, rowì— class labelì„ ë°°ì¹˜í•˜ê³  í•´ë‹¹ classifierë¥¼ trainì‹œ ì–´ë–¤ positive,negative valueë¥¼ ì‚¬ìš©í–ˆëŠ”ì§€ë¥¼ ê¸°ì…í•œë‹¤.
- ìƒì„±ëœ Mê°œì˜ classifierë“¤ì„ ì´ìš©í•˜ì—¬ test sampleì„ ë¶„ë¥˜í•˜ëŠ”ë°, ê°ê°ì˜ $C_n$ë“¤ì— ëŒ€í•´ distanceê°€ ê°€ì¥ ì‘ì€ labelì„ ê²°ê³¼ê°’ìœ¼ë¡œ í•œë‹¤. (í•´ë°ê±°ë¦¬ì™€ ìœ í´ë¦¬ë“œ ê±°ë¦¬ ì‚¬ìš©)
- classifier ìì²´ì˜ ì˜¤ë¥˜ë¥¼ ìˆ˜ì •í•˜ëŠ” error correction abilityê°€ ìˆìœ¼ë¯€ë¡œ (í•˜ë‚˜ì˜ classifierì˜ ì˜¤ë¥˜ ì •ë„ëŠ” í¬ê²Œ ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠìŒ) ECOCë¼ ë¶ˆë¦¼.
- ì´ë¡ ìƒ ì½”ë“œì˜ ê¸¸ì´($M$)ì´ ì¦ê°€í•˜ëŠ” ê²½ìš°, distance ì¦ê°€í•  ìˆ˜ë¡ ì„±ëŠ¥ì´ ì¢‹ì•„ì§.
- Ternary : neutralì€ í•´ë‹¹ classë¥¼ classifierê°€ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ë‹¤ëŠ” ê²ƒ

## 3.6 Class Imbalance Problem

- balanced classì˜ ê²½ìš°
	- $\text{odds : }\frac{y}{1-y}>1$ ì´ë©´, ì–‘ì„±ìœ¼ë¡œ íŒë³„
- unbalanced classì˜ ê²½ìš°
	- $\frac{y}{1-y}>\frac{m^+}{m^-}$ì´ë©´ ì–‘ì„±ìœ¼ë¡œ íŒë³„
- ê´€ì¸¡ odds(ratio of positive samples / negative samples)ì™€ ë¹„êµí•˜ëŠ” ê²ƒì´ íƒ€ë‹¹

### Rescaling

- ì¼ë°˜ì ìœ¼ë¡œ balanced classì— ëŒ€í•œ ì‹ì„ ì‚¬ìš©í•´ì•¼ í•˜ëŠ”ë°, unbalancedì˜ ê²½ìš° oddsì˜ ë¹„ìœ¨ì„ ê³ ë ¤í•´ ì¤˜ì•¼ í•˜ë¯€ë¡œ,

{% raw %}
$$
\frac{y'}{1-y'} = \frac{y}{1-y} \times \frac{m^-}{m^+}
$$
{% endraw %}


1. undersampling : negative samplesë¥¼ ì„ íƒì ìœ¼ë¡œ ì œê±°í•˜ì—¬ balanced classë¡œ ë§Œë“œëŠ” ë°©ë²•
2. oversampling : positive samples ìˆ˜ë¥¼ ëŠ˜ë ¤ì„œ balanced classë¡œ ë§Œë“œëŠ” ë°©ë²•
3. threshold-moving : ëª¨ë“  ìƒ˜í”Œì„ ê·¸ëŒ€ë¡œ í™œìš©í•˜ì§€ë§Œ, thresholdë¥¼ ë°”ê¾¸ì–´ (ìœ„ ì‹ ì²˜ëŸ¼) ì‚¬ìš©í•˜ëŠ” ë°©ë²•

<script>
  window.MathJax = {
    tex: {
      macros: {
        R: "\\mathbb{R}",
        N: "\\mathbb{N}",
        Z: "\\mathbb{Z}",
        Q: "\\mathbb{Q}",
        C: "\\mathbb{C}",
        proj: "\\operatorname{proj}",
        rank: "\\operatorname{rank}",
        im: "\\operatorname{im}",
        dom: "\\operatorname{dom}",
        codom: "\\operatorname{codom}",
        argmax: "\\operatorname*{arg\,max}",
        argmin: "\\operatorname*{arg\,min}",
        "\{": "\\lbrace",
        "\}": "\\rbrace",
        sub: "\\subset",
        sup: "\\supset",
        sube: "\\subseteq",
        supe: "\\supseteq"
      },
      tags: "ams",
      strict: false, 
      inlineMath: [["$", "$"], ["\\(", "\\)"]],
      displayMath: [["$$", "$$"], ["\\[", "\\]"]]
    },
    options: {
      skipHtmlTags: ["script", "noscript", "style", "textarea", "pre"]
    }
  };
</script>
<script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
