---
layout: post
date: 2023-09-30
title: "[ë¨¸ì‹ ëŸ¬ë‹] Chap 3 - ë¦¬ë‹ˆì–´ ëª¨ë¸"
tags: [ml, from_velog, abstract, ]
categories: [Machine Learning, ml_lecture, ]
media_subpath: assets/img/2023-09-30-[ë¨¸ì‹ ëŸ¬ë‹]-Chap-3---ë¦¬ë‹ˆì–´-ëª¨ë¸.md
image:
  path: 0.png
  alt: diagram explaining LDA
description: ë¨¸ì‹ ëŸ¬ë‹ì˜ ë¦¬ë‹ˆì–´ ëª¨ë¸ì— ëŒ€í•œ ë‚´ìš©ìœ¼ë¡œ, ê¸°ë³¸ í˜•ì‹, ì„ í˜• íšŒê·€, ë¡œì§€ìŠ¤í‹± íšŒê·€, ì„ í˜• íŒë³„ ë¶„ì„(LDA), ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ ë° í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œë¥¼ ë‹¤ë£¹ë‹ˆë‹¤. ì£¼ìš” ê°œë…ìœ¼ë¡œëŠ” MSE ìµœì†Œí™”, ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜, ìµœëŒ€ ìš°ë„ ì¶”ì •, LDAì˜ ë¶„ì‚° í–‰ë ¬, ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ ì „ëµ(OvO, OvR, MvM) ë° í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œ í•´ê²° ë°©ë²•ì´ í¬í•¨ë©ë‹ˆë‹¤.
pin: false
---


> ğŸ“£ Disclaimer : ë³¸ í¬ìŠ¤íŠ¸ëŠ” ì¡°ìš°ì¯”í™”ì˜ [ë‹¨ë‹¨í•œ ë¨¸ì‹ ëŸ¬ë‹](https://product.kyobobook.co.kr/detail/S000001916959) ì±…ì„ ìš”ì•½ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤. 


{: .prompt-info }



## Chap 3. Linear Model



### 3.1 ê¸°ë³¸ í˜•ì‹

- dê°œì˜ ì†ì„±ì„ ê°€ì§„ ìƒ˜í”Œ $x=(x_1, x_2, x_3, \cdots , x_d)$

> $$ f({\bf x}) = w_1x_1 + w_2x_2 + \cdots + w_dx_d + b$$  
> $$ f({\bf x}) = {\bf w^T x }+ b$$

- í•™ìŠµì„ í†µí•´ wì™€ bë¥¼ ê²°ì •. (ê³„ìˆ˜ ê²°ì •)


### 3.2 Linear regression

- eg) í•˜ë‚˜ì˜ propertyë§Œì„ ê°€ì§„ Data set Dë¥¼ ê³ ë ¤í•˜ì

> $$\text{consider } D = \{(x_1, y_1), \cdots, (x_m,y_m)\}$$

- Linear regression aims to learn the function:

> $$f(x) = wx+b, \text{s.t. } f(x_i)\approx y_i \text{i=1~m}$$  
> $$(w^, b^) = argmin_{\substack{(w,b)}}\sum_{i=1}^m (f(x_i)-y_i)^2$$

- Minimize the mean-square error(MSE)

> $$E_{(w,b)} = \sum_{i=1}^m (y_i-wx_i-b)^2$$  
> $$\frac{\partial E_{(w,b)}}{\partial w}=2\bigg(w\sum_{i=1}^m x_i^2-\sum_{i=1}^m(y_i-b)x_i\bigg)$$  
> $$\frac{\partial E_{(w,b)}}{\partial b}= 2\bigg(mb-\sum_{i=1}^m(y_i-wx_i)\bigg)$$  
> $$\therefore w = \frac{\sum_{i=1}^m y_i(x_i-\bar x)}{\sum_{i=1}^m x_i^2-\frac{1}{m}\bigg(\sum_{i=1}^m x_i\bigg)^2}$$  
> $$ b = \frac{1}{m}\sum_{i=1}^m(y_i-wx_i)$$

- Multivariate (${\bf x} = (x_1, x_2, \cdots, x_d)$)
- Datasetì„ matrix formìœ¼ë¡œ ë‚˜íƒ€ë‚´ ë³´ì

> $$ X = \begin{pmatrix}  
> x_{11} & x_{12} & \cdot & x_{1d} & 1 \\  
> x_{21} & x_{22} & \cdot & x_{2d} & 1 \\  
> \vdots & \vdots & \ddots & \vdots & \vdots \\  
> x_{m1} & x_{m2} & \cdots & x_{md} & 1 \end{pmatrix} =  \begin{pmatrix}  
> x_1^T & 1 \\  
> x_2^T & 1 \\  
> \vdots & \vdots\\  
> x_m^T & 1 \end{pmatrix}$$  
> $$ \text{when }\hat{\bf w} = ({\bf w};b)$$  
> $$\hat{\bf w}^* = argmin_{\hat{\bf w}} ({\bf y - X\hat w^T})({\bf y - X\hat w})$$  
> $$\frac{\partial E_{\hat{\bf w}}}{\partial {\hat{\bf w}}}= 2{\bf X^T(X\hat w - y)}$$

- The solution of multivariate problem

> $$ {\bf \hat{w}^*} = {\bf (X^T X)^{-1}X^T y}$$  
> $$ f({\bf \hat{x_i}})= {\bf \hat{x_i}}^T {\bf (X^T X)^{-1} X^T y}$$

- Log linear regression (special case of generalized linear models)
	- Dataê°€ ì§€ìˆ˜ ì²™ë„ì—ì„œ ë³€í™”í•œë‹¤ë©´ ì‚¬ìš©

	> $$\ln y = {\bf w^T x}+ b$$  
	> $$ y = g^{-1}({\bf w^T x + b})$$

- for generalized linear models, $g(\cdot )$ is monotonic differentible function (link function)
- $g(\cdot) = \ln(\cdot)$


### 3.3 Logistic Regression



#### Binary Classification

- Linear modelì„ ì‚¬ìš©í•˜ëŠ”ë°, ground truthê°€ $\{0,1\}$ì¸ ê²½ìš°
- $z = {\bf w^T x }+ b \in R$ ì´ë¯€ë¡œ, ì—°ê²°í•´ì¤„ link functionì„ ì°¾ì•„ì•¼ í•¨.
- Unit-step function(Heaviside function)

> $$ x = \begin{cases}  0, & z<0;\\ 1/2, & z=0;\\1, & z>0\end{cases}$$

- Unit step functionì€ link functionìœ¼ë¡œ í™œìš©í•  ìˆ˜ ì—†ìŒ (ë¯¸ë¶„ë¶ˆê°€ëŠ¥í•œì ì´ ì¡´ì¬í•˜ì—¬ $g^{-1}$ì´ ì •ì˜ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ)
- Sigmoid function

> $$y=\frac{1}{1+e^{-x}}$$

- sigmoid functionì€ differentibleí•˜ê³  monotoicí•˜ë¯€ë¡œ, ì´ë¥¼ ë³€í™˜ì‹œì¼œì£¼ë©´,

> $$\ln{\frac{y}{1-y}}= {\bf w^T x }+ b$$

- y : $x>0$ì¼ ê°€ëŠ¥ì„±, 1-y : $x<0$ì¼ ê°€ëŠ¥ì„±ìœ¼ë¡œ, ln í•­ì—ì„œ ë‘˜ì„ ë¹„êµí•˜ëŠ” ê²ƒìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŒ. ì´ëŸ¬í•œ ë¶„ìˆ˜ë¥¼ oddsë¼ê³  ë¶€ë¦„

> $$\text{odds} := \frac{y}{1-y}$$

- oddsì— ë¡œê·¸ë¥¼ ì·¨í•œ ê²ƒì„ logitì´ë¼ ë¶€ë¦„

> $$\text{logit} := \ln{\frac{y}{1-y}}$$



#### applying maximum likelihood

- logits can be rewritten as

> $$\ln{\frac{p(y=1\vert {\bf x})}{p(y=0\vert {\bf x})}} = {\bf w^T x }+ b$$  
> $$ p(y=1\vert {\bf x}) = \frac{e^{w^Tx+b}}{1+e^{w^T x + b}}$$  
> $$ p(y=0\vert {\bf x}) = \frac{1}{1+e^{w^T x + b}}$$

- Maximum likelihood methodë¥¼ í†µí•´ $w, b$ë¥¼ ì¶”ì¸¡í•˜ë©´,

> $$l({\bf w},  b) = \sum_{i=1}^m \ln {p(y_i \vert {\bf x_i}; {\bf w}, b)}$$

- ì´ ë•Œ $\beta = ({\bf w};b), \hat{\bf x} = ({\bf x}; 1)$ ë¡œ ê°€ì •,

> $$l({\bf w},  b) = \sum_{i=1}^m \ln{y_ip_1(\hat {\bf x_i};\beta)+ (1-y_i)p_0(\hat {\bf x_i};\beta)}$$  
> $$ l({\bf \beta}) = \sum_{i=1}^m \bigg(-y_i{\bf \beta^T}\hat{\bf x_i} + \ln{\big(1+e^{{\bf \beta^T}\hat{\bf x_i}}\big)}\bigg)$$

- find beta with gradient descent, Newton method

> $${\bf \beta^*} = argmin_{\beta} l(\beta)$$

- eg) Newton's Method,

> $${\bf \beta}^{t+1} = {\bf \beta}^t - \bigg(\frac{\partial^2 l(\beta)}{\partial \beta\partial \beta^T}\bigg) \frac{\partial l(\beta)}{\partial \beta}$$  
> $$\frac{\partial l(\beta)}{\partial \beta} = \sum_{i=1}^m \hat{\bf x_i} (y_i-p_1(\hat{\bf x_i};{\bf \beta}))$$  
> $$\bigg(\frac{\partial^2 l(\beta)}{\partial \beta\partial \beta^T}\bigg) = \sum_{i=1}^m \hat{\bf x_i}\hat{\bf x_i}^T p_1(\hat{\bf x_i};{\bf \beta}) (1-p_1(\hat{\bf x_i};{\bf \beta}))$$



### 3.4 Linear Discriminant Analysis (LDA)


![0](/0.png)

- Data pointsë¥¼ $y=wx+b$ì— íˆ¬ì˜ì‹œí‚¤ê³ , ê°™ì€ labelì„ ê°€ì§„ ì ë“¤ì˜ ìœ„ì¹˜ê°€ ìµœëŒ€í•œ ê°€ê¹ë„ë¡ wì™€ bë¥¼ ì°¾ëŠ” ë°©ë²•
- ì¦‰ data setì´ givenì¼ ë•Œ ìµœëŒ€í™”ì‹œí‚¤ëŠ” $y=wx+b$ë¥¼ ì°¾ëŠ” ê²ƒ
- Some variables (n: number of features, $m_i$ : number of data)

> sample set of the ith class $X_i$ ($n\times m_i$)  
> mean vector of the ith class ${\bf \mu_i}$ ($n \times 1$)  
> covariance matrix of the i-th class ${\bf \Sigma_i}$ ($n \times n$) : ê°ê°ì˜ ë³€ìˆ˜ë“¤ì— ëŒ€í•œ ê³µë¶„ì‚°ì„ matrix í˜•íƒœë¡œ í‘œì‹œ

- After projection to a line

> the centers of those two classes samples ${\bf w}^T{\bf \mu}_0$, ${\bf w}^T{\bf \mu}_1$  
> the covariances of the two classes samples ${\bf w}^T{\bf \Sigma}_0{\bf w}$,  ${\bf w}^T{\bf \Sigma}_1{\bf w}$

- Objective to be maximized

> $$ J = \frac{\Vert{\bf w^T \mu_0 - w^T \mu_1}\Vert_2^2}{\bf w^T{\bf \Sigma_0}{\bf w}+ \bf w^T{\bf \Sigma_1}{\bf w}}$$

- ë¶„ìì™€ ë¶„ëª¨ë¡œ ë‚˜ëˆ„ì–´ ìƒê°í•´ë³´ë©´ ë¶„ìëŠ” í‰ê· ì˜ 2-norm, ë¶„ëª¨ëŠ” co-variance ì˜ í•©ì„ì„ ì•Œ ìˆ˜ ìˆë‹¤.

> $$J = \frac{ \bf w^T({\bf \mu_0-\mu_1})({\bf \mu_0-\mu_1})^T{\bf w}}{\bf w^T({\bf \Sigma_0 +\Sigma_1}){\bf w}}$$

- The within-class scatter matrix

> $$ {\bf S_w} = {\bf \Sigma_0 + \Sigma_1}$$  
> $$ =\sum_{{\bf x}\in X_0}({\bf x-\mu_0})({\bf x-\mu_0})^T+ \sum_{{\bf x}\in X_1}({\bf x-\mu_1})({\bf x-\mu_1})^T$$

- Between-class scatter matrix

> $$ {\bf S_b} =({\bf \mu_0-\mu_1})({\bf \mu_0-\mu_1})^T$$

- Generalized Rayleigh quotient

> $$ J = \frac{\bf w^T S_b w}{\bf w^T S_w w}$$

- ${\bf w^T S_w w} = 1$ì´ë¼ í•˜ë©´, maximizing generalized Reyleigh quotient is equivalent to

> $$ min_w -{\bf w^T S_b w}$$  
> $$ \text{s.t.} {\bf w^T S_w w}=1$$

- Using the method of Lagrange multipliers (ì €ëŸ° constraint ë¬¸ì œë¥¼ í‘¸ëŠ” ë°©ë²• : lagrangian multiplier)

> $${\bf S_b w} = \lambda {\bf S_w w}$$

- recap the definition of Between class scatter matrix

> $${\bf S_b w} = \lambda ({\bf \mu_0-\mu_1})$$

- ìœ„ ë‘ ì‹ì„ ì—°ë¦½í•˜ë©´,

> $${\bf w} = {\bf S_w^{-1}} ({\bf \mu_0-\mu_1})$$

- Solve by singular value decomposition ${\bf S_w} = {\bf U\Sigma V^T}$


#### Extend LDA to multiclass classification problems

- The global scatter matrix

> $${\bf S_t} ={\bf S_b} + {\bf S_w}$$  
> $$=\sum_{i=1}^m ({\bf x_i-\mu})({\bf x_i-\mu})^T$$

- Decomposition of within-class scatter matrix

> $$ {\bf S_w} = \sum_{i=1}^N {\bf S_{w_i}}$$  
> where $$ S_{w_i} = \sum_{{\bf x}\in X_i} ({\bf x-\mu_i})({\bf x-\mu_i})^T$$  
> $${\bf S_b} = {\bf S_t}-{\bf S_w} = \sum_{i=1}^N m_i ({\bf \mu_i-\mu})({\bf \mu_i-\mu})^T$$

- Think about the formula $Var(X) = E(X^2)-E(X)^2$
- Objective :

> $$max_{\bf W}\ \ \frac{tr\bigg({\bf W^T S_b W}\bigg)}{tr\bigg({\bf W^T S_w W}\bigg)}$$  
> $$\text{where }{\bf W}\in {\bf R}^{d\times (N-1)}$$  
> (trace?) : ê°ê°ì— ëŒ€í•œ w vectorë“¤ì€ ì„œë¡œì„œë¡œë¼ë¦¬ë§Œ ê³±í•´ì ¸ì•¼í•˜ë¯€ë¡œ ê´€ì‹¬ìˆëŠ” í•­ì€ diagonalì„.  
> ì—¬ê¸°ì„œì˜ d : n (number of features), N : number of classes

- which leads to lagrange multiplier problem

> $$ {\bf S_B W} = \lambda {\bf S_w W}$$

- W corresponds to d' largest non-zero eigenvalues of ${\bf S_w^{-1} S_b}$ (d'ê°œì˜ non-zero eigenvaluesë¡œë¶€í„° êµ¬í•œ eigenvectorë¥¼ concatí•œ ê²Œ Wì´ë‹¤?) where $d'\leq N-1$


### 3.5 Multiclass Classification

- Nê°œ í´ë˜ìŠ¤ë¥¼ ë¶„ë¥˜í•˜ëŠ” ë¶„í•´ë²•. Binary classificationì˜ í™•ì¥ìœ¼ë¡œ, 3ê°€ì§€ ì •ë„ì˜ ì „í˜•ì  ë¶„í•´ ì „ëµì´ ìˆìŒ
- OvO (One vs. One)
- OvR (One vs. Rest)
- MvM (Many vs. Many)


#### OvO (One vs One classification)

- Nê°œì˜ í´ë˜ìŠ¤ì—ì„œ 2ê°œì”© ì„ íƒí•¨. ì¦‰ $_nC_2$ê°œì˜ Binary classification ë¬¸ì œ ìƒì„±
- ê²°ê´ê°’ì˜ ê²°ì •ì€ ê°€ì¥ ë§ì€ ì„ íƒì„ ë°›ì€ ê²°ê³¼ë¥¼ ì„ íƒí•˜ê²Œ ë¨.


#### OvR (One vs Rest classification)

- Nê°œì˜ í´ë˜ìŠ¤ ê°ê°ì„ ì–‘ì„±ìœ¼ë¡œ, ê·¸ ë‚˜ë¨¸ì§€ë¥¼ ìŒì„±ìœ¼ë¡œ í•˜ì—¬ Nê°œì˜ classifier í•™ìŠµ.
- í•œê°œë§Œ ì–‘ì„±ì´ë¼ë©´ ê·¸ labelì„, ì—¬ëŸ¬ ê°œê°€ ì–‘ì„±ì´ë¼ë©´ ê° classifierì˜ ì˜ˆì¸¡ ì‹ ë¢°ë„ê°€ ê°€ì¥ í° í´ë˜ìŠ¤ì˜ ë ˆì´ë¸”ì„ ê²°ê³¼ë¡œ ì„ íƒ
- OvO, OvRì€ ì—¬ëŸ¬ tradeoffê°€ ìˆì§€ë§Œ ê²°ê³¼ì ìœ¼ë¡œ ì„±ëŠ¥ì€ ë¹„ìŠ·í•¨


#### MvM (Many vs Many classification)

- Nê°œì˜ classë“¤ì„ Më²ˆ positiveì™€ negative (í˜¹ì€ neutral)ë¡œ ë‚˜ëˆ”.
- base codewordë¥¼ ê³„ì‚°.

![1](/1.png)

- columnì— ê° classifierë“¤ì„, rowì— class labelì„ ë°°ì¹˜í•˜ê³  í•´ë‹¹ classifierë¥¼ trainì‹œ ì–´ë–¤ positive,negative valueë¥¼ ì‚¬ìš©í–ˆëŠ”ì§€ë¥¼ ê¸°ì…í•œë‹¤.
- ìƒì„±ëœ Mê°œì˜ classifierë“¤ì„ ì´ìš©í•˜ì—¬ test sampleì„ ë¶„ë¥˜í•˜ëŠ”ë°, ê°ê°ì˜ $C_n$ë“¤ì— ëŒ€í•´ distanceê°€ ê°€ì¥ ì‘ì€ labelì„ ê²°ê³¼ê°’ìœ¼ë¡œ í•œë‹¤. (í•´ë°ê±°ë¦¬ì™€ ìœ í´ë¦¬ë“œ ê±°ë¦¬ ì‚¬ìš©)
- classifier ìì²´ì˜ ì˜¤ë¥˜ë¥¼ ìˆ˜ì •í•˜ëŠ” error correction abilityê°€ ìˆìœ¼ë¯€ë¡œ (í•˜ë‚˜ì˜ classifierì˜ ì˜¤ë¥˜ ì •ë„ëŠ” í¬ê²Œ ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠìŒ) ECOCë¼ ë¶ˆë¦¼.
- ì´ë¡ ìƒ ì½”ë“œì˜ ê¸¸ì´(M)ì´ ì¦ê°€í•˜ëŠ” ê²½ìš°, distance ì¦ê°€í•  ìˆ˜ë¡ ì„±ëŠ¥ì´ ì¢‹ì•„ì§.
- Ternary : neutralì€ í•´ë‹¹ classë¥¼ classifierê°€ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ë‹¤ëŠ” ê²ƒ


### 3.6 Class Imbalance Problem

- balanced classì˜ ê²½ìš°

> $\text{odds : }\frac{y}{1-y}>1$ ì´ë©´, ì–‘ì„±ìœ¼ë¡œ íŒë³„

- unbalanced classì˜ ê²½ìš°

> $\frac{y}{1-y}>\frac{m^+}{m^-}$ì´ë©´ ì–‘ì„±ìœ¼ë¡œ íŒë³„

- ê´€ì¸¡ odds(ratio of positive samples / negative samples)ì™€ ë¹„êµí•˜ëŠ” ê²ƒì´ íƒ€ë‹¹


#### Rescaling

- ì¼ë°˜ì ìœ¼ë¡œ balanced classì— ëŒ€í•œ ì‹ì„ ì‚¬ìš©í•´ì•¼ í•˜ëŠ”ë°, unbalancedì˜ ê²½ìš° oddsì˜ ë¹„ìœ¨ì„ ê³ ë ¤í•´ ì¤˜ì•¼ í•˜ë¯€ë¡œ,

> $\frac{y'}{1-y'} = \frac{y}{1-y} \times \frac{m^-}{m^+}$

1. undersampling : negative samplesë¥¼ ì„ íƒì ìœ¼ë¡œ ì œê±°í•˜ì—¬ balanced classë¡œ ë§Œë“œëŠ” ë°©ë²•
2. oversampling : positive samples ìˆ˜ë¥¼ ëŠ˜ë ¤ì„œ balanced classë¡œ ë§Œë“œëŠ” ë°©ë²•
3. threshold-moving : ëª¨ë“  ìƒ˜í”Œì„ ê·¸ëŒ€ë¡œ í™œìš©í•˜ì§€ë§Œ, thresholdë¥¼ ë°”ê¾¸ì–´ (ìœ„ ì‹ ì²˜ëŸ¼) ì‚¬ìš©í•˜ëŠ” ë°©ë²•
