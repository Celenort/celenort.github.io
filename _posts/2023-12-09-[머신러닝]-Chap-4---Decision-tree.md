---
layout: post
date: 2023-12-09
title: "[ë¨¸ì‹ ëŸ¬ë‹] Chap 4 - Decision tree"
categories: [Machine Learning, ml_lecture, ]
media_subpath: assets/img/2023-12-09-[ë¨¸ì‹ ëŸ¬ë‹]-Chap-4---Decision-tree.md/
image:
  path: 0.png
  alt: A Decision tree
description: ë¨¸ì‹ ëŸ¬ë‹ì˜ ê²°ì • íŠ¸ë¦¬ ì•Œê³ ë¦¬ì¦˜ì€ ë°ì´í„°ë¥¼ ë¶„í• í•˜ê¸° ìœ„í•´ ì •ë³´ ì—”íŠ¸ë¡œí”¼, ì´ë“ ë¹„ìœ¨, ì§€ë‹ˆ ì§€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©°, ê³¼ì í•© ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì „í›„ ê°€ì§€ì¹˜ê¸°ë¥¼ ì ìš©í•©ë‹ˆë‹¤. ì—°ì† ê°’ê³¼ ê²°ì¸¡ê°’ ì²˜ë¦¬ ë°©ë²•ë„ ì„¤ëª…ë˜ë©°, ë‹¤ë³€ëŸ‰ ê²°ì • íŠ¸ë¦¬ì˜ ê°€ëŠ¥ì„±ì— ëŒ€í•´ì„œë„ ë…¼ì˜ë©ë‹ˆë‹¤.
pin: false
---


### Disclaimer


{: .prompt-info }


> ğŸ“£ ë³¸ í¬ìŠ¤íŠ¸ëŠ” ì¡°ìš°ì¯”í™”ì˜ [ë‹¨ë‹¨í•œ ë¨¸ì‹ ëŸ¬ë‹](https://product.kyobobook.co.kr/detail/S000001916959) ì±…ì„ ìš”ì•½ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤. 


# Chap 4. Decision tree


## 4.1 Basic process

- Decision Tree Learning algorithm
- Input : Testing set $D$, Feature set $A$

Process :

1. generate node $i$
2. **if** $\forall D \in C$ :
	- Mark node $i$ as a class $C$ leaf node, **return**
3. **if** $A=\varnothing \ \text{or} \ \forall D
$ take some value on $A$ :
	- Mark node $i$ as a leaf node, label = majority class in $D$, **return**
4. Set optimal splitting feature $a^*$ from $A$ (Entropy, GINI, etc...)
5. **for** each value $a_{*}^{v}$  in  $a_{*}$, **do** :
	1. Generate branch for node $i$ ; $D_v$ is subset of samples taking $a_{*}^{v} \text{on } a_{*}$
	2. **if** $D_v$ is empty :
		- Mark $D_v$ as leaf node, label it with majority class in $D$, **return**
	3. **else** :
		- $D_v$ = self($D_v$, $A \ \backslash \ \{a_*\}$)

Output : Decision tree with root node $i$


## 4.2 Split selection


### Split by information entropy

- Information Entropy (or simply entropy)

{% raw %}
$$
\begin{aligned}Ent(D) = -\sum_{k=1}^{f} p_k \log_2 p_k
\end{aligned}
$$
{% endraw %}

- $f$ : Number of features
- Entropy ì˜ë¯¸ : í´ ìˆ˜ë¡ ì˜ ë¶„ë°°ë˜ì–´ ìˆëŠ” ê²ƒ. 0ì— ê°€ê¹Œìš¸ ìˆ˜ë¡ í•œìª½ìœ¼ë¡œ í¸í–¥ë˜ì–´ ìˆë‹¤ëŠ” ëœ» (ì‘ì„ ìˆ˜ë¡ ìˆœë„ê°€ ë†’ìŒ)
- Gain of splitting samples by feature

{% raw %}
$$
Gain(D, feature) = Ent(D)-\sum_{k=1}^{V}\frac{\vert D^v\vert}{\vert D \vert} Ent(D^v)
$$
{% endraw %}

- ëª¨ë“  ë‚¨ì•„ìˆëŠ” featureì— ëŒ€í•œ information gainì„ ê³„ì‚°í•˜ì—¬ ê°€ì¥ gainì´ ë†’ì€ (ì—”íŠ¸ë¡œí”¼ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì¤„ì¼ ìˆ˜ ìˆëŠ”) featureë¥¼ íƒí•˜ì—¬ ë¶„í• ì„ ì§„í–‰

### Split by gain ratio

- Gain ratio : Gain ìì²´ê°€ ì»¤ì§€ê²Œ í•˜ë ¤ë©´ attributeì˜ ìˆ˜ê°€ ë§ì€ featureê°€ ìœ ë¦¬í•˜ê²Œ ì‘ë™í•¨. ì¦‰ ì´ì— ëŒ€í•œ ë³´ì •ì„ í•˜ëŠ” ê²ƒì´ gain ratio ë³´ì •(?)

{% raw %}
$$
\begin{aligned}
	\text{Gain ratio} \ (D, a) =& \ \frac{Gain(D,a)}{IV(a)} \\ 
	IV(a) =& \   -\sum_{k=1}^{V} \frac{\vert D^v\vert}{\vert D \vert} \log_2 \frac{\vert D^v\vert}{\vert D \vert}
	\end{aligned}
$$
{% endraw %}

- Gain ratio is biased towards features with fewer possible values : possible values ìˆ˜ê°€ ë§ì€ ê²½ìš° (like case of ID) ì €í‰ê°€í•¨.

### Split by GINI Index

- Gini(D) : ì„ì˜ì˜ 2ê°œ sampleì´ ì„œë¡œ ë‹¤ë¥¸ í´ë˜ìŠ¤ì¼ í™•ë¥ . (Gini(D)ê°€ í´ ìˆ˜ë¡ Dì˜ purityëŠ” ë†’ìŒ)

{% raw %}
$$
\begin{aligned}Gini(D) =& \  \sum_{k=1}^{N(attr)}\sum_{k'\neq k}p_k p_{k'} \\ =& \ 1-\sum_{k=1}^{N(attr)}p_k^2
	
	
	\end{aligned}
$$
{% endraw %}

- Gini indexê°€ ê°€ì¥ ì‘ì€ featureë¥¼ ê³ ë¦„

{% raw %}
$$
\text{Gini index}(D,a) = \sum_{v=1}^V \frac{\vert D^v\vert}{\vert D \vert} Gini(D^v)
$$
{% endraw %}


## 4.3 Pruning

- Deal with overfitting.
- General strategies : pre/post-pruning
- First, using hold-out method, split dataset into training and validation set

### Pre-pruning


![0](/0.png)

- comparing before/after splitting
- ê°€ì§€ì¹˜ê¸°ë¥¼ í•˜ê¸° ì „ì— splitting ê³¼ì •ì—ì„œë¶€í„° pre-pruningì„ ì§„í–‰í•˜ëŠ” ê²ƒ.
- validation setì— ëŒ€í•œ accruacyë¥¼ splitting ì „í›„ë¡œ ê³„ì‚°, ê·¸ rateê°€ ëŠ˜ì–´ë‚˜ì§€ ì•Šìœ¼ë©´ prune, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ split
- Advantages : Reduce the risk of overfitting, computational cost of training and testing
- Disadvantages : Risk of underfitting

### Post-pruning

- ì´ë¯¸ ë§Œë“¤ì–´ì§„ decision treeì— ëŒ€í•œ 'post' pruning
- ë¨¼ì € total treeì— ëŒ€í•œ accruacyë¥¼ ê³„ì‚°.
- í•˜ìœ„ nodeë¶€í„°, pruning ì „í›„ì˜ accuracy ì°¨ì´ë¥¼ ê³„ì‚°, íš¨ìš©ì´ ìˆë‹¤ë©´ pruning ì§„í–‰. (ê°™ë‹¤ë©´ ì§„í–‰ x)
- Advantages : less prone to underfitting -> better generalization ability
- Disadvantages : training time of post-pruning is much longer (í•˜ìœ„ë…¸ë“œë¶€í„° í•˜ë‚˜ì”© ì „ë¶€ ë”°ì ¸ë´ì•¼ í•˜ë¯€ë¡œ)

## 4.4 Continuous values & missing values


### Continuous values

- Discetization Stretagy (Bi-partition) : Dataset Dì—ì„œ feature aì— ëŒ€í•´ të¥¼ ê¸°ì¤€ìœ¼ë¡œ 2ê°œì˜ groupìœ¼ë¡œ ë‚˜ëˆ”.
- ì´ ë•Œ Split pointì˜ í›„ë³´ ë¶„í• ì ì´ ë˜ëŠ” ì 

{% raw %}
$$
T_a = \{\frac{a^i+a^{i+1}}{2}\} \text{ for 1 }\leq i \leq n-1
$$
{% endraw %}

- Midpoint $T_a$ê°€ ì‚¬ìš©ë˜ë©°, ì´ë¥¼ ì´ìš©í•´ êµ¬í•œ Gainì€ ë‹¤ìŒê³¼ ê°™ìŒ.

{% raw %}
$$
\begin{aligned}Gain(D,a) =& \  \max_{t\in T_a} Gain(D,a,t) \\ =& \  max\ Ent(D)-\sum_{\lambda\in\{-,+\}} \frac{\vert D_t^\lambda\vert}{\vert D\vert} Ent(D_t^\lambda)\end{aligned}
$$
{% endraw %}

- aë¼ëŠ” propertyë¥¼ ì´ìš©í•´ Dë¥¼ ë¶„ë¦¬í•  ë•Œ të¼ëŠ” ìƒˆë¡œìš´ ë³€ìˆ˜ê¹Œì§€ ê³ ë ¤í•´ì„œ gainì´ ìµœëŒ€í™”ë˜ëŠ” të¥¼ ì„ íƒí•˜ê² ë‹¤ëŠ” ëœ».

### Missing Values

- To solve problems :
	1. featureì˜ ì„ íƒ
	2. featureê°€ ì„ íƒëœ ê²½ìš° ìƒ˜í”Œì´ ê²°ì¸¡ê°’ì´ë¼ë©´ ì–´ë–»ê²Œ ë¶„í• í•  ê²ƒì¸ê°€.
- íŠ¹ì • feature $a$ì— ëŒ€í•˜ì—¬:
	- $w_x$ : weight for sample $x$
	- $D$ : total data
	- $\tilde D$ : data that has values of $a$
	- $\tilde D_k$ : $k$th sample(classifierâ€™s ground truth)
- $\rho$ : ì „ì²´ Data ì¤‘ $a$ì˜ ë°ì´í„°ê°€ ì¡´ì¬í•˜ëŠ” ë¹„ìœ¨ (weighted)

{% raw %}
$$
\rho = \frac{\sum_{x\in \tilde D} w_x}{\sum_{x\in D} w_x}
$$
{% endraw %}

- $\tilde p_k$ : ê²°ì¸¡ê°’ ì—†ëŠ” ë°ì´í„° ì¤‘ label : $k$ì˜ ë¹„ìœ¨ (weighted)

{% raw %}
$$
\tilde p_k = \frac{\sum_{x\in \tilde D_k} w_x}{\sum_{x\in \tilde D} w_x}
$$
{% endraw %}

- $\tilde r_v$ : ê²°ì¸¡ê°’ ì—†ëŠ” ë°ì´í„° ì¤‘ feature $a$ì— ëŒ€í•´ì„œ $v$ì˜ ê°’ì„ ê°–ëŠ” ë¹„ìœ¨ (weighted)

{% raw %}
$$
\tilde r_v = \frac{\sum_{x\in \tilde D^v} w_x}{\sum_{x\in \tilde D} w_x}
$$
{% endraw %}

- Gain, Entropyì˜ ì¬ì •ì˜

{% raw %}
$$
\begin{aligned}Gain(D,a) =& \  \rho \times Gain(\tilde D, a) \\ =& \  \rho \times \bigg(Ent(\tilde D) - \sum_{v=1}^V \tilde r_v Ent(\tilde D^v)\bigg) \\ 
	Ent(\tilde D) =& \  -\sum_{k=1}^{N(attr)} \tilde p_k \log_2\tilde p_k
	\end{aligned}
$$
{% endraw %}

- ë§Œì•½ feature $a$ë¡œ ë¶„í•  í–ˆëŠ”ë° íŠ¹ì • sample $x$ì˜ feature $a$ê°€ ê²°ì¸¡ê°’ì´ë¼ë©´, $x$ë¥¼ ëª¨ë“  í•˜ìœ„ nodeì— ë™ì¼í•˜ê²Œ ê·€ì†ì‹œí‚¤ëŠ” ëŒ€ì‹  ê·¸ ê°€ì¤‘ì¹˜(weight)ì„ $w_x$ì—ì„œ $\tilde r_v w_x$ë¡œ updateí•œë‹¤.

## 4.5 Multivariate Decision Trees

- Feature axisì— data pointë¥¼ plot í•˜ì˜€ì„ ë•Œ Decision treeê°€ í•˜ëŠ” ì¼ì€ axisì— parallelí•œ ê²½ê³„ì„ ì„ ê¸‹ëŠ” í–‰ìœ„.
- Parallelí•˜ì§€ ì•Šì€ ëŒ€ê°ì„  lineì„ ê·¸ë¦´ ìˆ˜ ìˆë‹¤ë©´?

![1](/1.png)

