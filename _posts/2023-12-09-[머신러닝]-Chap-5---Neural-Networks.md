---
layout: post
date: 2023-12-09
title: "[ë¨¸ì‹ ëŸ¬ë‹] Chap 5 - Neural Networks"
tags: [Perceptron, ]
categories: [Lecture, Machine Learning, ]
media_subpath: /assets/img/2023-12-09-[ë¨¸ì‹ ëŸ¬ë‹]-Chap-5---Neural-Networks.md
image:
  path: 0.png
  alt: A Decision tree
description: ë¨¸ì‹ ëŸ¬ë‹ì˜ ê²°ì • íŠ¸ë¦¬ ì•Œê³ ë¦¬ì¦˜ì€ ë°ì´í„°ë¥¼ ë¶„í• í•˜ê¸° ìœ„í•´ ì •ë³´ ì—”íŠ¸ë¡œí”¼, ì´ë“ ë¹„ìœ¨, ì§€ë‹ˆ ì§€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©°, ê³¼ì í•© ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì „í›„ ê°€ì§€ì¹˜ê¸°ë¥¼ ì ìš©í•©ë‹ˆë‹¤. ì—°ì† ê°’ê³¼ ê²°ì¸¡ê°’ ì²˜ë¦¬ ë°©ë²•ë„ ì„¤ëª…ë˜ë©°, ë‹¤ë³€ëŸ‰ ê²°ì • íŠ¸ë¦¬ì˜ ê°€ëŠ¥ì„±ì— ëŒ€í•´ì„œë„ ë…¼ì˜ë©ë‹ˆë‹¤.
pin: false
---


### Disclaimer


{: .prompt-info }


> ğŸ“£ ë³¸ í¬ìŠ¤íŠ¸ëŠ” ì¡°ìš°ì¯”í™”ì˜ [ë‹¨ë‹¨í•œ ë¨¸ì‹ ëŸ¬ë‹](https://product.kyobobook.co.kr/detail/S000001916959) ì±…ì„ ìš”ì•½ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤. 


# Chap 5. Neural Network


## 5.1 Neuron Model


### M-P Neuron model

- Input : receive input signals from $n$ neurons
- Process : weighted sum of received signals is compared against the threshold
- Output : signal produced by activation function i.e.) $\text{output = } f({\bf w^T x + b})$

![0](/0.png)


### Activation function

- Unit step function (ì–‘ìˆ˜ì´ë©´ 1, ìŒìˆ˜ì´ë©´ 0)
- sigmoid function ($\sigma(x) = \frac{1}{1+e^{-x}}$) : unit step functionì€ ë¯¸ë¶„ë¶ˆê°€ëŠ¥ì„±, ë¶ˆì—°ì†ì„± ë“±ì˜ ì¢‹ì§€ ì•Šì€ ì¡°ê±´ìœ¼ë¡œ ì¸í•´ sigmoidë¥¼ ì‚¬ìš©.
- $(-\infty, +\infty)$ì˜ ê°’ì„ $[0,1]$ë¡œ ë°€ì–´ë„£ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ squashing functionì´ë¼ ë¶ˆë¦¬ê¸°ë„ í•œë‹¤.

## 5.2 Perceptron and Multi-layer Networks

- Perceptron : 2ê°œ layer (input, output)ìœ¼ë¡œ êµ¬ì„±, input layerëŠ” inputì„ ë°›ì•„ì„œ output layerì— ì „ë‹¬, output layerëŠ” M-P neuronìœ¼ë¡œ threshold ì—°ì‚° ìˆ˜í–‰.

![1](/1.png)

- Perceptronìœ¼ë¡œ Logic Gateë¥¼ êµ¬í˜„ ê°€ëŠ¥ (AND, OR, NAND) : by tuning $w_1, w_2, \theta$
- Activation functionì„ USFë¡œ ê°€ì •í•˜ë©´,
- AND : $w_1, w_2 = 1, \theta = 2$ : if $x_1, x_2$ are all 1, then output is 1
- OR : $w_1 = 1, w_2 = 1, \theta = 0.5$: if one of $x_1, x_2$$is 1, then output is 1
- NOT : $w_1 = -0.6, w_2 = 0, \theta = -0.5$ : if $x_1$ is 1, then output is 0, else otherwise

### Perceptron learning

- weight $w_i$ and threshold $\theta$ can be learned from training data
- thresholdë¥¼ ìƒìˆ˜ì— ëŒ€í•œ weightë¡œ ë³´ë©´ ëª¨ë“  learning processë¥¼ weightì— ëŒ€í•œ updateë¡œ ìƒê°í•  ìˆ˜ ìˆìŒ.

{% raw %}
$$
\begin{aligned}
w_i &\leftarrow w_i + \Delta w_i \\
\Delta w_i &= \eta (y-\hat y) x_i \\ 
\eta \in (0,1) &\text{: Learning rate}
\end{aligned}
$$
{% endraw %}

- (Single layer) Perceptronì€ Linearly seperable problemì— ëŒ€í•œ í•™ìŠµë§Œì´ ê°€ëŠ¥. if XOR (00, 11 = 0, 01,11=1) ê°™ì€ íšŒë¡œëŠ” í•˜ë‚˜ì˜ ì„ ìœ¼ë¡œ ë¶„ë¥˜ê°€ ë¶ˆê°€ëŠ¥í•˜ë¯€ë¡œ í•™ìŠµì´ ì•ˆë¨ (but it is guaranteed to converge for linear seperable ploblems)
- ì œí•œì  í•™ìŠµ ëŠ¥ë ¥

### Multi-layer perceptrons

- Hidden layer : Activation functionì´ ì ìš©ë˜ëŠ” input layerì™€ output layer ì‚¬ì´ì˜ layer.
- Linear seperable problemì´ ì•„ë‹Œ ê²½ìš°ì—ë„ ë¬¸ì œ í•´ê²°ì´ ê°€ëŠ¥.

## 5.3 Error Backpropagation Algorithm

- BP (error Back Propagation)
- Constraints(Datas)

{% raw %}
$$
\begin{aligned}
\text{Given dataset } D &= \{({\bf x_i}, y_i)\}, {\bf x_i}\in R^d, y_i \in R^l\ (i=1,2,\cdots, m) \\
\theta_j &\text{ : threshold of the jth output neuron} \\
\gamma_h &\text{ : threshold of the hth hidden neuron} \\
v_{ih} &\text{ : connection weight btw input - hidden} \\
w_{hj} &\text{ : connection weight btw hidden - output}
\end{aligned}
$$
{% endraw %}


![2](/2.png)

- Consider a single sample $\bf x_i$ to be trained at Multi-layered network with 1 hidden layer
- Hidden Layerì˜ output $b_i$ :

{% raw %}
$$
\begin{aligned}
b_i &= f(\alpha_h - \gamma_h) \\ \alpha_h &= \sum_{i=1}^d v_{ih} x_i
\end{aligned}
$$
{% endraw %}

- Ouptut layerì˜ output $\hat y_j^k$ :

{% raw %}
$$
\begin{aligned}
\hat y_j ^k &= f(\beta_j -\theta_j) \\ \beta_j &= \sum_{i=1}^q w_{hj}b_h\end{aligned}
$$
{% endraw %}

- Data index $k$, ($\bf x_k$) ì— ëŒ€í•œ sum of squared(?) error :

{% raw %}
$$
E_k = \frac{1}{2} \sum_{j=1}^l (\hat y_j^k - y_j^k)^2
$$
{% endraw %}

- Parameters to tune : $v_{ih}, w_{hj}, \theta_j, \gamma_h$
- How to tune : update $v$ with delta v

{% raw %}
$$
\Delta w_hj = -\eta \frac{\partial E_k}{\partial w_hj}
$$
{% endraw %}

- ê° ë³€ìˆ˜ì— ëŒ€í•œ DeltaëŠ” $E_k$ë¥¼ object functionìœ¼ë¡œ ë³´ê³  $E_k$ë¥¼ ë‚´ê°€ tune í•˜ê³ ì í•˜ëŠ” ë³€ìˆ˜ì— ëŒ€í•œ í¸ë¯¸ë¶„ì„ í•œ ë‹¤ìŒ í•™ìŠµë¥  $\eta$ì™€ -1ì„ ê³±í•œ ê°’ìœ¼ë¡œ Deltaë¥¼ ì •í•˜ì—¬ ì—…ë°ì´íŠ¸ í•œë‹¤. (ê¸°ìš¸ê¸°ëŠ” ì¦ê°€í•˜ëŠ” ë°©í–¥ì´ ì•„ë‹ˆë¼ ì¤„ì–´ë“œëŠ” ë°©í–¥ìª½ìœ¼ë¡œ ì°¾ì•„ì•¼ Minimumì„ ì°¾ìœ¼ë‹ˆê¹Œ -ë¥¼ ë¶™ì„.) ì´ ë•Œ ê³„ì‚°ì€ ê³„ì‚° ìˆœì„œì— ë”°ë¼ chain ruleì„ ì ìš©,
- object functionì—ëŠ” sigmoid functionì„ ì‚¬ìš©í–ˆë‹¤ê³  ê°€ì •. $\hat y_j^k$ë‚˜, hidden layerì˜ outputì„ ë¯¸ë¶„í•  ë•Œì—ëŠ” weighted sumì´ ì•„ë‹Œ, functionì•ˆì— weighted sumì´ ë“¤ì–´ê°„ í˜•íƒœë¥¼ ì‚¬ìš©í•˜ê²Œ ë˜ë¯€ë¡œ function ìì²´ì˜ derivativeë¥¼ ì·¨í•´ì£¼ëŠ” ê³¼ì •, ì†ë¯¸ë¶„ ê³¼ì • ìŠì§€ë§ì•„ì•¼.
- Derivation process

![3](/3.png)


### Accumulated BP Algorithm

- minimizes the accumulated error on the whole training set

{% raw %}
$$
E = \frac{1}{m} \sum_{k=1}^m E_k
$$
{% endraw %}

- Tunes parameter less frequently.
- ì€ë‹‰ì¸µ ë‰´ëŸ° ê°œìˆ˜ëŠ” trial-and-errorë°–ì— ì—†ìŒ

### How to overcome OVERFITTING

- Early stopping : training error ê°ì†Œ, validation error ì¦ê°€í•˜ëŠ” ìˆœê°„ terminate
- Regularization : recall tikhonov regulatization (also minimizes the squared sum of weights)

## 5.4 Global / Local minimum

- find $\bf w^\ast, \theta^\ast$ s.t. gradient of E is zero : local minimum
- í•­ìƒ local minimumì´ 1ê°œ(local=global)ì„ì´ ë³´ì¥ë˜ì§€ëŠ” ì•ŠìŒ

### How to "jump out" from the local minimum

- Simulated annealing (ë‹´ê¸ˆì§ˆ ê¸°ë²•) : ì¼ì •í™œë¥ ë¡œ í˜„ì¬ í•´ë³´ë‹¤ ë‚˜ìœ ê²°ê³¼ë¡œ ì´ë™. (ì°¨ì„ ì˜ í•´ë¥¼ ë°›ì„ í™•ë¥ ì´ ì ì  ì¤„ì–´ë“¤ê²Œ ë˜ë©´ì„œ, ì‹œê°„ì— ë”°ë¼ algorithmì´ ì•ˆì •)
- Stochastic gradient descent : ê¸°ìš¸ê¸° ê³„ì‚°ì‹œ ëœë¤ìš”ì†Œë¥¼ ì¶”ê°€í•˜ì—¬, local minimumì—ì„œ ê³„ì‚°ê°’ì´ 0ì´ ì•„ë‹ ìˆ˜ë„ ìˆìŒ.
- Genetic algorithm : ë¹„ìŠ·..

## 5.5 Other common neural networks

- NOT considered in this lecture

## 5.6 Deep learning

- NOT considered in this lecture
