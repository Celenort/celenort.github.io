---
layout: post
title: "[ì œì–´ê³µí•™ê°œë¡ ] Lec 06 - Basic Linear Algebra"
date: 2023-12-09
draft: false
published: true
pin: false
image:
  path: ""
  alt: "[ì œì–´ê³µí•™ê°œë¡ ] Lec 06 - Basic Linear Algebra"
description: "ì œì–´ê³µí•™ê°œë¡  ê°•ì˜ì—ì„œëŠ” í–‰ë ¬-ë²¡í„° ë° í–‰ë ¬-í–‰ë ¬ ê³±ì…ˆì˜ ì „ê°œ, ì„ í˜• ë°©ì •ì‹ì˜ í•´ì˜ ì¡´ì¬ì„±ê³¼ ìœ ì¼ì„±, í–‰ë ¬ì˜ ì—­ìˆ˜, ìŠˆë¥´ ì—¬ì¸ìˆ˜, ê³ ìœ ê°’ ë° ê³ ìœ ë²¡í„°, ëŒ€ê°í™” ì¡°ê±´ ë“±ì„ ë‹¤ë£¨ë©°, íŠ¹íˆ ëŒ€ê°í™”ê°€ ê°€ëŠ¥í•œ ì¡°ê±´ê³¼ ì œì–´ ì´ë¡ ì—ì„œì˜ ì‘ìš©ì— ëŒ€í•´ ì„¤ëª…í•©ë‹ˆë‹¤."
tags: ["Linear Algebra"]
categories: ["Lecture", "ì œì–´ê³µí•™ê°œë¡ "]
math: true
---


## ğŸ“¢Precaution


{: .prompt-info}


> ë³¸ ê²Œì‹œê¸€ì€ ì„œìš¸ëŒ€í•™êµ ì‹¬í˜•ë³´ êµìˆ˜ë‹˜ì˜ 23-2 ì œì–´ê³µí•™ê°œë¡  ìˆ˜ì—… ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.


## Row/Columnwise expansion of matrix-vector multiplication


{% raw %}
$$
Ax = \begin{bmatrix}\vert & \vert & & \vert \\ a_{\cdot 1} & a_{\cdot 2} & \cdots & a_{\cdot n} \\ \vert & \vert &  & \vert\end{bmatrix}\begin{bmatrix}\vert \\ x\\ \vert\end{bmatrix} = x_1 \begin{bmatrix}\vert \\ a_{\cdot 1}\\ \vert\end{bmatrix} + \cdots + x_n \begin{bmatrix}\vert \\ a_{\cdot n}\\ \vert\end{bmatrix}
$$
{% endraw %}



{% raw %}
$$
\begin{aligned}x^TB &= \begin{bmatrix}- & x & -\end{bmatrix}\begin{bmatrix}- & b_{1\cdot} & - \\ - & b_{2\cdot} & -\\ & \vdots &  \\- & b_{n\cdot} & -\end{bmatrix}  \\ &= x_1^T  \begin{bmatrix}- & b_{1\cdot} & - \end{bmatrix} + \cdots + x_n^T \begin{bmatrix}- & b_{n\cdot} & -\end{bmatrix}\end{aligned}
$$
{% endraw %}



## Row/Columnwise expansion of matrix-matrix multiplication


with the same method,


{% raw %}
$$
\begin{aligned}
A[B,C] &= [AB, AC] \\ 
\begin{bmatrix}A \\ B \end{bmatrix} C &= \begin{bmatrix}AC \\ BC \end{bmatrix}
\end{aligned}
$$
{% endraw %}



## two ways to describe Vectorspace

- ì œí•œí˜•

{% raw %}
$$
\{x\in\Re^3 \ \vert \  [0,0,1]\cdot x = 0\}
$$
{% endraw %}


- ìƒì„±í˜•

{% raw %}
$$
\{span{\bigg[\begin{bmatrix}1 \\ 0\\ 0\end{bmatrix}, \begin{bmatrix}0 \\ 1\\ 0\end{bmatrix}, \begin{bmatrix}0 \\0\\ 1\end{bmatrix}\bigg]}\}
$$
{% endraw %}



## solve Ax=b (linear equation)

- í•´ì˜ ì¡´ì¬ì„± : Aì˜ columnë“¤ì´ bë¥¼ spaní•  ìˆ˜ ìˆë‹¤ë©´, í•´ê°€ ì¡´ì¬. because....

{% raw %}
$$
\begin{aligned} Ax &= x_1A_1 + x_2A_2 + \cdots+x_nA_n = b \\ b &\in span(A) = R(A)
	\end{aligned}
$$
{% endraw %}


- í•´ì˜ ìœ ì¼ì„± : Aì˜ nullspaceê°€ ê³µì§‘í•©ì¸ ê²½ìš°

{% raw %}
$$
N(A) = 0, N(A):=\{x|Ax=0\}
$$
{% endraw %}


- proof)

{% raw %}
$$
\begin{aligned}
	A\bar x = b, Ax^*=b \\ 
	A(\bar x - x^*) = 0 \Leftrightarrow \bar x = x^*
	\end{aligned}
$$
{% endraw %}



### "full column/row rank"


rank(A) = number of columns (i.e. $ N(A)=0$)


## Inverse of a matrix


InverseëŠ” Aê°€ ì •ë°©í–‰ë ¬ì¼ ë•Œ ì¡´ì¬.


{% raw %}
$$
\begin{aligned}
A^{-1} := AA^{-1} = A^{-1}A=I \\
Ax = [Ax_1, Ax_2, \cdots, Ax_n] = [e_1, e_2, \cdots e_n]
\end{aligned}
$$
{% endraw %}



solving N linear equations


{% raw %}
$$
\begin{aligned}
Ax_1 &= e_1, Ax_2 = e_2 \cdots, Ax_n = e_N \\
R(A) &\in {e_1, e_2, \cdots, e_n} \\
N(A) &= {0}
\end{aligned}
$$
{% endraw %}



Aì˜ Inverseì˜ columnë“¤ ì—­ì‹œ ì„ í˜• ë…ë¦½ì´ì–´ì•¼ í•¨.


{% raw %}
$$
\begin{aligned}
A(c_1x_1+c_2x_2 + \cdots +c_nx_n) = 0, \text{then,} \\
\text{RHS =} c_1e_1 + c_2e_2 + \cdots c_ne_n = 0 \\
\text{only holds if } c_1 = c_2 = \cdots =c_n=0
\end{aligned}
$$
{% endraw %}



Inverseì˜ ì¡´ì¬ì„±ì€ $det(A)\neq0$

- det A : divisionì—°ì‚°ì„ ë¯¸í¬í•¨, ë¬¼ë¦¬ì ìœ¼ë¡œ Volumeê³¼ ì¼ë§¥ìƒí†µ
- Volumeì˜ ì¡´ì¬ì„± : nê°œì˜ ë²¡í„°ë“¤ì´ ì„œë¡œ ë…ë¦½ì´ ì•„ë‹ ê²½ìš° det=0ì„.

## Schur complement


{% raw %}
$$
\begin{aligned}
&det\begin{bmatrix}A & B \\ C & D\end{bmatrix} \\
A, B &= \text{rectangular matrix} \\
&\text {if A invertible,} \\
&=det A * det(D-CA^{-1}B) \\
&\text {if D invertible,} \\
&= det D * det(A-BD^{-1}C
\end{aligned}
$$
{% endraw %}


- ABCDì—ì„œ Aê°€ invertibleí•˜ë©´ Dë¡œ ê°€ê³  ì´í›„ ì‹œê³„ë°©í–¥.
- Dê°€ invertibleí•˜ë©´, Aë¡œ ê°€ê³  ì´í›„ ì‹œê³„ë°©í–¥

## Eigenvalue, Eigenvector


{% raw %}
$$
\lambda \text{s.t. } Ax=\lambda x
$$
{% endraw %}



{% raw %}
$$
\lambda, x = \text{(eigenvalue, eigenvector)}
$$
{% endraw %}



{% raw %}
$$
\begin{aligned} (\lambda I - A) x &= 0 \\ det(\lambda I - A)&= 0\end{aligned}
$$
{% endraw %}


- $\lambda$ì— ëŒ€í•œ Characteristic polynomial.
- C.Pì˜ coeffê°€ Realì´ë©´ eigenvalueë„ real or complex conjugate
- Real / Complex eigenvalue/vector

{% raw %}
$$
\begin{aligned}
	\lambda\text{ in } R \rightarrow x\in R^n \\ 
	\lambda\text{ in } C \rightarrow x\in C^n \\
	Ax=\lambda x, A\bar x = \bar \lambda \bar x
	\end{aligned}
$$
{% endraw %}



### Companion form

- Matrix that preserves Characteristic Polynomial (ë§ˆì§€ë§‰ rowì— ëª¨ë“  coeffë“¤ì´ ë“¤ì–´ê°€ ìˆìŒ.)
- ì°¨ìˆ˜ëŠ” ì˜¤ë¦„ì°¨ìˆœìœ¼ë¡œ (ì°¨ìˆ˜ ë‚®ì€ê±°ë¶€í„°)

## Diagonalization of matrix


given matrix $A \in R^{n\times n}$


eigenvalues of $\lambda_i$ and corrosponding eigenvector $v_i$


{% raw %}
$$
v_i \in N(\lambda_i I -A)
$$
{% endraw %}



does $v_i$ s are independent?


if all $\lambda_i$ are distinct, all $v_i$ are linearly independent

- algebraic multiplicity : ì‹¤ì œë¡œ ëª‡ë²ˆ $\lambda_i$ê°€ ì¤‘ì²©ë˜ì—ˆëŠ”ê°€.
- geometric multiplicity : $dim\bigg(N(\lambda_i I -A)\bigg)$

### Geometric Multiplicity = Algebraic Multiplicity


ì•…ì˜ì ìœ¼ë¡œ ì„¤ì •í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´, ê²¹ì¹˜ì§€ ì•ŠëŠ” $v_i$ë“¤ì„ ì°¾ì„ ìˆ˜ ìˆìŒ.


{% raw %}
$$
\begin{aligned}
\text{if }dim(N(\lambda_i I -A)) = 2 \\
v_i, v_j \in N(\lambda_i I -A), \text{s.t.} v_i \neq kv_j
\end{aligned}
$$
{% endraw %}



### Algebraic Multiplicity > Geometric Multiplicity


diagonal matrixì—ì„œ upper triangleì— 1ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°, **degeneracy**ê°€ ìƒê¹€.


Algebraic multiplicityê°€ 2ì¸ë° geometric multiplicityê°€ 1ì¸ ê²½ìš° : í•´ë‹¹ eigenvalue(ë“¤) ë¡œë¶€í„° ë‹¨ 1ê°œì˜ ë…ë¦½ì¸ ë²¡í„°ë¥¼ ë½‘ì•„ë‚¼ ìˆ˜ ì—†ìŒ.


### nê°œì˜ ë…ë¦½ì¸ eigenvectorë¥¼ ë½‘ì•„ë‚¼ ìˆ˜ ìˆëŠ” ì¡°ê±´ (ì¶©ë¶„ì¡°ê±´)

1. all $\lambda_i$ are distinct
2. $A=A^T$ : symmetric (ì•…ì˜ì ì´ì§€ ì•Šê²Œ ì„¤ì •í•˜ë©´)

### Diagonalization


Nê°œì˜ ë…ë¦½ì¸ eigenvectorë“¤ì„ ì°¾ì„ ìˆ˜ ìˆëƒ ì—†ëƒì— ë”°ë¼ diagonalization ê°€ëŠ¥ ì—¬ë¶€ê°€ ê²°ì •ë¨


{% raw %}
$$
\begin{aligned}
Av_1 &= \lambda_1 v_1 \\
Av_i &= \lambda_i v_i \\
Av_n &= \lambda_n v_n
\end{aligned}
$$
{% endraw %}



Diagonalization of A


{% raw %}
$$
\begin{aligned}
&AV = VD \\
&A[v_1, v_2, \cdots, v_n] = diag(\lambda_1, \lambda_2, \cdots, \lambda_n)[v_1, v_2, \cdots, v_n]
\end{aligned}
$$
{% endraw %}



if $v$ are invertible (all columns of v are linearly independent,)


{% raw %}
$$
\begin{aligned}
V^{-1}AV &= D\\ 
VDV^{-1} &= A \text{(PDPinverse)}
\end{aligned}
$$
{% endraw %}



Application to control theory :


{% raw %}
$$
\begin{aligned}
\dot x &= Ax \\ 
\dot z &= Dz \\
\text{where } z&=V^{-1}x
\end{aligned}
$$
{% endraw %}



since $D$ is diagonal matrix, easy to calculate $e^{At}$ (scalar for each row)


<script>
  window.MathJax = {
    tex: {
      macros: {
        R: "\\mathbb{R}",
        N: "\\mathbb{N}",
        Z: "\\mathbb{Z}",
        Q: "\\mathbb{Q}",
        C: "\\mathbb{C}",
        proj: "\\operatorname{proj}",
        rank: "\\operatorname{rank}",
        im: "\\operatorname{im}",
        dom: "\\operatorname{dom}",
        codom: "\\operatorname{codom}",
        argmax: "\\operatorname*{arg\,max}",
        argmin: "\\operatorname*{arg\,min}",
        "\{": "\\lbrace",
        "\}": "\\rbrace",
        sub: "\\subset",
        sup: "\\supset",
        sube: "\\subseteq",
        supe: "\\supseteq"
      },
      tags: "ams",
      strict: false, 
      inlineMath: [["$", "$"], ["\\(", "\\)"]],
      displayMath: [["$$", "$$"], ["\\[", "\\]"]]
    },
    options: {
      skipHtmlTags: ["script", "noscript", "style", "textarea", "pre"]
    }
  };
</script>
<script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
