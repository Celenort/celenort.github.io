---
title: 'ML study - Optimization'
layout: single
categories:
  - teach
  - math1
tags: []
mathjax: true
---

## Optimization

### Key components to be defined : 

- Decision variables
- Objective function
- Constraints

### In mathematical Model

- $$ min_{x} f(x) \\ subject \ to \ g_i(x) \leq 0, \ \ i=1, \cdots, m $$
- $ x = [x_1, x_2, \cdots, x_n]^T  \in \real^n$ is decision variable
- $ f : \real^n  \in \real$
- Feasible Region : $C \ : \ \{x \ | \ g_i (x)\leq 0, i=1, \cdots, m\}$
- $ x^* \in  \real ^n $ is optimal solution if $ x^* \in C$ and $f(x^*)\geq f(x), \forall x \in C$

### Solving Optimiaztion Problems

- Unconstrained problem with multiple decision variables
- The gradient of $f$ should be zero.

	- $$\nabla_x f(x) = 0 $$

- The gradient is an n-dimensional vector containing partial derivatives with respect to each dimension.

	- $$ \nabla_x f(x) = [{\partial f(x) \over \partial x_1}, \cdots, {\partial f(x) \over \partial x_n}]^T $$

- An optimal soluation $x^*$ exists when $f$ is continuous and differentible. 

	- $$\nabla_x f(x^*) = 0$$

### Numarical Solution

- Gradient decent method
	
	- Look for the direction of the steepest descent at $x_0$.
	- Update the search point with a step size of $\alpha > 0$
	- $$ x\leftarrow x-\alpha f'(x)$$
	- Repeat above steps until convergence.

- Step Size (or Learning Rate)

	- Too small : converge very slowly.
	- Too big : overshoot and even disverge
	- Reduce step size over time

- Local Minimum

	- Convex : Any local minimum is a global minimum (only one global(and local) minimum exists)
	- Non-convex : Multiple local minima may exist
	